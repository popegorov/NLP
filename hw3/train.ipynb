{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-09T13:10:57.052914Z",
     "iopub.status.busy": "2024-11-09T13:10:57.051913Z",
     "iopub.status.idle": "2024-11-09T13:10:57.096615Z",
     "shell.execute_reply": "2024-11-09T13:10:57.095027Z",
     "shell.execute_reply.started": "2024-11-09T13:10:57.052872Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, input_size: int, prob: float=1.0, eps: float=1e-8):\n",
    "        super().__init__()\n",
    "        assert prob > 0.0\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.gamma = nn.Parameter(torch.ones(input_size), requires_grad=True)\n",
    "        self.prob = prob\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input_: Tensor) -> Tensor:\n",
    "        if self.prob >= 1.0:\n",
    "            input_norm = input_.norm(2, dim=-1, keepdim=True)\n",
    "            den = input_norm / math.sqrt(self.input_size) + self.eps\n",
    "            return (input_ / den) * self.gamma\n",
    "\n",
    "        est_size = int(self.prob * self.input_size)\n",
    "        to_calc = input_[..., :est_size]\n",
    "        est_norm = to_calc.norm(2, dim=-1, keepdim=True)\n",
    "        den = est_norm / torch.sqrt(est_size) + self.eps\n",
    "        return (input_ / den) * self.gamma\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T13:10:57.100219Z",
     "iopub.status.busy": "2024-11-09T13:10:57.099010Z",
     "iopub.status.idle": "2024-11-09T13:10:57.118454Z",
     "shell.execute_reply": "2024-11-09T13:10:57.117315Z",
     "shell.execute_reply.started": "2024-11-09T13:10:57.100185Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class RoPE:\n",
    "    def __init__(self, seq_len: int, embed_size: int, device: torch.device):\n",
    "        self.base = 10000\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        arange = torch.arange(0, self.embed_size, 2, device=device)\n",
    "        theta = 1.0 / (self.base ** (arange / embed_size))\n",
    "        idxs = torch.arange(seq_len, device=device)\n",
    "\n",
    "        outer_product = torch.outer(theta, idxs)\n",
    "        self.storage = torch.stack((torch.cos(outer_product), torch.sin(outer_product)), dim=-1).to(device)\n",
    "\n",
    "    def apply(self, input_: torch.Tensor):\n",
    "        batch_size, seq_len, nheads, hidden_size = input_.shape\n",
    "        # cur_storage = self.storage[:seq_len]\n",
    "        x = input_.view(batch_size, seq_len, nheads, hidden_size // 2, 2)\n",
    "        cur_storage = self.storage.view(1, seq_len, 1, hidden_size // 2, 2)\n",
    "        \n",
    "        cos_theta = cur_storage[..., 0]\n",
    "        sin_theta = cur_storage[..., 1]\n",
    "        output = torch.stack([\n",
    "            x[..., 0] * cos_theta - x[..., 1] * sin_theta,\n",
    "            x[..., 0] * sin_theta + x[..., 1] * cos_theta,\n",
    "        ], dim=-1)\n",
    "\n",
    "        return output.view(batch_size, seq_len, nheads, hidden_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T13:10:58.150292Z",
     "iopub.status.busy": "2024-11-09T13:10:58.149173Z",
     "iopub.status.idle": "2024-11-09T13:10:58.168252Z",
     "shell.execute_reply": "2024-11-09T13:10:58.167096Z",
     "shell.execute_reply.started": "2024-11-09T13:10:58.150222Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Parameter(torch.tensor([1.0]), requires_grad=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, input_: Tensor) -> Tensor:\n",
    "        return input_ * self.sigmoid(self.beta * input_)\n",
    "\n",
    "\n",
    "class SwiGLUFeedForward(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, device):\n",
    "        super().__init__()\n",
    "        self.U = nn.Linear(input_size, hidden_size, bias=True, device=device)\n",
    "        self.W = nn.Linear(input_size, hidden_size, bias=True, device=device)\n",
    "        self.V = nn.Linear(hidden_size, input_size, bias=True, device=device)\n",
    "        self.swish = Swish(device)\n",
    "\n",
    "    def forward(self, input_: Tensor) -> Tensor:\n",
    "        x1 = self.swish(self.U(input_)) # batch_size, seq_len, hidden_dim * c\n",
    "        x2 = self.W(input_) # batch_size, seq_len, hidden_dim * c\n",
    "        x = self.V(x1 * x2)\n",
    "        return x # batch_size, seq_len, hidden_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T13:10:59.099333Z",
     "iopub.status.busy": "2024-11-09T13:10:59.098209Z",
     "iopub.status.idle": "2024-11-09T13:10:59.117771Z",
     "shell.execute_reply": "2024-11-09T13:10:59.116663Z",
     "shell.execute_reply.started": "2024-11-09T13:10:59.099284Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, seq_len: int, hidden_dim: int, n_head: int, rope: RoPE, device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.w_q = nn.Linear(hidden_dim, hidden_dim, bias=False, device=device)\n",
    "        self.w_k = nn.Linear(hidden_dim, hidden_dim, bias=False, device=device)\n",
    "        self.w_v = nn.Linear(hidden_dim, hidden_dim, bias=False, device=device)\n",
    "\n",
    "        self.shuffler = nn.Linear(hidden_dim, hidden_dim, bias=False, device=device)\n",
    "        self.rope = RoPE(seq_len, hidden_dim // n_head, device)\n",
    "\n",
    "    def forward(self, input_: torch.Tensor, mask: bool=True):\n",
    "        batch_size, seq_len, hidden_dim = input_.shape\n",
    "        head_size = hidden_dim // self.n_head\n",
    "\n",
    "        Q = self.w_q(input_).view(batch_size, seq_len, self.n_head, head_size)\n",
    "        K = self.w_k(input_).view(batch_size, seq_len, self.n_head, head_size)\n",
    "        V = self.w_v(input_).view(batch_size, seq_len, self.n_head, head_size)\n",
    "\n",
    "        Q = self.rope.apply(Q).transpose(1, 2)\n",
    "        K = self.rope.apply(K).transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        coefs = torch.softmax(torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(head_size), dim=-1) # batch_size, nhead, seq_len, seq_len\n",
    "        if mask:\n",
    "            coefs = torch.tril(coefs)\n",
    "\n",
    "        output = torch.matmul(coefs, V) # batch_size, nhead, seq_len, head_size\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_dim)\n",
    "        output = self.shuffler(output)\n",
    "\n",
    "        return output # batch_size, seq_len, hidden_dim\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T13:10:59.927747Z",
     "iopub.status.busy": "2024-11-09T13:10:59.926532Z",
     "iopub.status.idle": "2024-11-09T13:10:59.955933Z",
     "shell.execute_reply": "2024-11-09T13:10:59.954956Z",
     "shell.execute_reply.started": "2024-11-09T13:10:59.927710Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LLaMaBlock(nn.Module):\n",
    "    def __init__(self, seq_len:int, hidden_size: int, n_head: int, rope: RoPE, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.rms_attn = RMSNorm(hidden_size)\n",
    "        self.attention = MultiHeadAttention(seq_len, hidden_size, n_head, rope, device)\n",
    "        self.rms_ffn = RMSNorm(hidden_size)\n",
    "        self.swiglu = SwiGLUFeedForward(hidden_size, 8 * hidden_size // 3, device)\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.attention(self.rms_attn(input_)) # batch_size, seq_len, hidden_dim\n",
    "        x = input_ + x \n",
    "        x = x + self.swiglu(self.rms_ffn(x)) \n",
    "        return x # batch_size, seq_len, hidden_dim\n",
    "\n",
    "\n",
    "class LLaMa(nn.Module):\n",
    "    def __init__(self, vocab_size:int, n_stacks:int, seq_len: int, hidden_size: int, n_head: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        rope = RoPE(seq_len, hidden_size // n_head, device)\n",
    "        self.n_stacks = n_stacks\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size, device=device)\n",
    "        self.rmsnorm = RMSNorm(hidden_size)\n",
    "        self.blocks = nn.Sequential()\n",
    "        for i in range(n_stacks):\n",
    "            self.blocks.add_module(f\"LLaMa Block {i}\", LLaMaBlock(seq_len, hidden_size, n_head, rope, device))\n",
    "\n",
    "        self.output_linear = nn.Linear(hidden_size, vocab_size, bias=True, device=device)\n",
    "\n",
    "    def forward(self, input_: torch.Tensor):\n",
    "        x = self.embed(input_) # batch_size, seq_len, hidden_dim\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.rmsnorm(x) \n",
    "        x = self.output_linear(x) \n",
    "        return x # batch_size, seq_len, vocab_size\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Model prints with the number of parameters.\n",
    "        \"\"\"\n",
    "        all_parameters = sum([p.numel() for p in self.parameters()])\n",
    "        trainable_parameters = sum(\n",
    "            [p.numel() for p in self.parameters() if p.requires_grad]\n",
    "        )\n",
    "\n",
    "        result_info = super().__str__()\n",
    "        result_info = result_info + f\"\\nAll parameters: {all_parameters}\"\n",
    "        result_info = result_info + f\"\\nTrainable parameters: {trainable_parameters}\"\n",
    "\n",
    "        return result_info\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T13:11:01.128288Z",
     "iopub.status.busy": "2024-11-09T13:11:01.126823Z",
     "iopub.status.idle": "2024-11-09T13:11:01.154279Z",
     "shell.execute_reply": "2024-11-09T13:11:01.153212Z",
     "shell.execute_reply.started": "2024-11-09T13:11:01.128234Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install datasets\n",
    "# %pip install wandb\n",
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T13:11:02.990485Z",
     "iopub.status.busy": "2024-11-09T13:11:02.989787Z",
     "iopub.status.idle": "2024-11-09T13:11:06.519782Z",
     "shell.execute_reply": "2024-11-09T13:11:06.518449Z",
     "shell.execute_reply.started": "2024-11-09T13:11:02.990436Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: timtorbakhov111 (popegorov). Use `wandb login --relogin` to force relogin\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n",
      "wandb: Tracking run with wandb version 0.18.6\n",
      "wandb: Run data is saved locally in /home/jupyter/work/resources/wandb/run-20241109_131105-mpmuoqkg\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run winter-rain-5\n",
      "wandb: ⭐️ View project at https://wandb.ai/popegorov/uncategorized\n",
      "wandb: 🚀 View run at https://wandb.ai/popegorov/uncategorized/runs/mpmuoqkg\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key='')\n",
    "run = wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T13:11:09.667332Z",
     "iopub.status.busy": "2024-11-09T13:11:09.665502Z",
     "iopub.status.idle": "2024-11-09T13:11:31.028866Z",
     "shell.execute_reply": "2024-11-09T13:11:31.027622Z",
     "shell.execute_reply.started": "2024-11-09T13:11:09.667256Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size 1000000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.tokenized_data = dataset.map(tokenize, batched=True, remove_columns=['text'])['input_ids']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tokenized_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "\n",
    "\n",
    "def compute_loss(criterion, logits: torch.Tensor, labels: torch.Tensor, pad_id):\n",
    "    logits = logits.reshape(-1, 32000)\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    loss = criterion(logits, labels, ignore_index=pad_id)\n",
    "    return loss\n",
    "\n",
    "\n",
    "counter = 0\n",
    "PAD_ID = 2\n",
    "MAX_SEQ_LEN = 256\n",
    "login(token=\"\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "\n",
    "    \n",
    "dataset_id = \"ashaba1in/small_openwebtext\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "print(\"Dataset size\", len(dataset['train']['text']))\n",
    "# print(dataset['train']['text'][0])\n",
    "tokenizer_id =\"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "PAD_ID = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T13:11:31.031757Z",
     "iopub.status.busy": "2024-11-09T13:11:31.030760Z",
     "iopub.status.idle": "2024-11-09T13:11:31.057707Z",
     "shell.execute_reply": "2024-11-09T13:11:31.056460Z",
     "shell.execute_reply.started": "2024-11-09T13:11:31.031690Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    flatten = []\n",
    "    for text in batch:\n",
    "        flatten += text\n",
    "    target_size = len(batch) * MAX_SEQ_LEN\n",
    "    if len(flatten) < target_size:\n",
    "        flatten += [PAD_ID] * (target_size - len(flatten))\n",
    "    else:\n",
    "        flatten = flatten[:target_size]\n",
    "    \n",
    "    flatten = torch.Tensor(flatten).to(torch.long).view(len(batch), MAX_SEQ_LEN)\n",
    "    return flatten\n",
    "\n",
    "\n",
    "def train_epoch(model, criterion, optimizer, train_loader, epoch, pad_id, log_step, save_period, run):\n",
    "    loss_log = []\n",
    "    model.train()\n",
    "    counter = 0\n",
    "    log_count = 0\n",
    "    for data in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        data = data.to(device) # batch_size, seq_len\n",
    "        counter += 1\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data) # batch_size, seq_len, vocab_size\n",
    "        loss = compute_loss(criterion, out[:, :-1], data[:, 1:].clone(), pad_id)\n",
    "        loss_log.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if not counter % log_step:\n",
    "            print(f\"Loss from {log_count * log_step} to {(log_count + 1) * log_step} step\", np.mean(loss_log[log_count * log_step: (log_count + 1) * log_step]))\n",
    "            run.log({'loss': np.mean(loss_log[log_count * log_step: (log_count + 1) * log_step])})\n",
    "            log_count += 1\n",
    "            \n",
    "        if not counter % save_period:\n",
    "            torch.save(model, \"best_model.pth\")\n",
    "            wandb.save(\"best_model.pth\")\n",
    "\n",
    "    return loss_log\n",
    "\n",
    "def train(model, criterion, optimizer, n_epochs, pad_id, log_step, save_period, run, dataset):\n",
    "    print(model)\n",
    "    len_epoch = 100000\n",
    "    for epoch in range(n_epochs):\n",
    "        cropped_dataset = dataset['train'].select(range(epoch * len_epoch, (epoch + 1) * len_epoch))\n",
    "        my_data = MyDataset(cropped_dataset)\n",
    "        train_loader = DataLoader(my_data, batch_size=8, collate_fn=collate_fn)\n",
    "        \n",
    "        train_loss = train_epoch(model, criterion, optimizer, train_loader, epoch, pad_id, log_step, save_period, run)\n",
    "        print(f\"Train loss: {np.mean(train_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T13:12:27.301664Z",
     "iopub.status.busy": "2024-11-09T13:12:27.300235Z",
     "iopub.status.idle": "2024-11-09T19:57:11.074725Z",
     "shell.execute_reply": "2024-11-09T19:57:11.039019Z",
     "shell.execute_reply.started": "2024-11-09T13:12:27.301610Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMa(\n",
      "  (embed): Embedding(32000, 768)\n",
      "  (rmsnorm): RMSNorm()\n",
      "  (blocks): Sequential(\n",
      "    (LLaMa Block 0): LLaMaBlock(\n",
      "      (rms_attn): RMSNorm()\n",
      "      (attention): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (shuffler): Linear(in_features=768, out_features=768, bias=False)\n",
      "      )\n",
      "      (rms_ffn): RMSNorm()\n",
      "      (swiglu): SwiGLUFeedForward(\n",
      "        (U): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (W): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (V): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (swish): Swish(\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (LLaMa Block 1): LLaMaBlock(\n",
      "      (rms_attn): RMSNorm()\n",
      "      (attention): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (shuffler): Linear(in_features=768, out_features=768, bias=False)\n",
      "      )\n",
      "      (rms_ffn): RMSNorm()\n",
      "      (swiglu): SwiGLUFeedForward(\n",
      "        (U): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (W): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (V): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (swish): Swish(\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (LLaMa Block 2): LLaMaBlock(\n",
      "      (rms_attn): RMSNorm()\n",
      "      (attention): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (shuffler): Linear(in_features=768, out_features=768, bias=False)\n",
      "      )\n",
      "      (rms_ffn): RMSNorm()\n",
      "      (swiglu): SwiGLUFeedForward(\n",
      "        (U): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (W): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (V): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (swish): Swish(\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (LLaMa Block 3): LLaMaBlock(\n",
      "      (rms_attn): RMSNorm()\n",
      "      (attention): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (shuffler): Linear(in_features=768, out_features=768, bias=False)\n",
      "      )\n",
      "      (rms_ffn): RMSNorm()\n",
      "      (swiglu): SwiGLUFeedForward(\n",
      "        (U): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (W): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (V): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (swish): Swish(\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (LLaMa Block 4): LLaMaBlock(\n",
      "      (rms_attn): RMSNorm()\n",
      "      (attention): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (shuffler): Linear(in_features=768, out_features=768, bias=False)\n",
      "      )\n",
      "      (rms_ffn): RMSNorm()\n",
      "      (swiglu): SwiGLUFeedForward(\n",
      "        (U): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (W): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (V): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (swish): Swish(\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (LLaMa Block 5): LLaMaBlock(\n",
      "      (rms_attn): RMSNorm()\n",
      "      (attention): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (shuffler): Linear(in_features=768, out_features=768, bias=False)\n",
      "      )\n",
      "      (rms_ffn): RMSNorm()\n",
      "      (swiglu): SwiGLUFeedForward(\n",
      "        (U): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (W): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (V): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (swish): Swish(\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (LLaMa Block 6): LLaMaBlock(\n",
      "      (rms_attn): RMSNorm()\n",
      "      (attention): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (shuffler): Linear(in_features=768, out_features=768, bias=False)\n",
      "      )\n",
      "      (rms_ffn): RMSNorm()\n",
      "      (swiglu): SwiGLUFeedForward(\n",
      "        (U): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (W): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (V): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (swish): Swish(\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (LLaMa Block 7): LLaMaBlock(\n",
      "      (rms_attn): RMSNorm()\n",
      "      (attention): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (shuffler): Linear(in_features=768, out_features=768, bias=False)\n",
      "      )\n",
      "      (rms_ffn): RMSNorm()\n",
      "      (swiglu): SwiGLUFeedForward(\n",
      "        (U): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (W): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (V): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (swish): Swish(\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_linear): Linear(in_features=768, out_features=32000, bias=True)\n",
      ")\n",
      "All parameters: 105859080\n",
      "Trainable parameters: 105859080\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc52d42f12a141bb9b007cd9b9ed90d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss from 0 to 20 step 8.42400221824646\n",
      "Loss from 20 to 40 step 7.0235237121582035\n",
      "Loss from 40 to 60 step 6.822653651237488\n",
      "Loss from 60 to 80 step 6.586639356613159\n",
      "Loss from 80 to 100 step 6.394759893417358\n",
      "Loss from 100 to 120 step 6.401404309272766\n",
      "Loss from 120 to 140 step 6.304096984863281\n",
      "Loss from 140 to 160 step 6.292648983001709\n",
      "Loss from 160 to 180 step 6.172917222976684\n",
      "Loss from 180 to 200 step 6.16012670993805\n",
      "Loss from 200 to 220 step 6.07097475528717\n",
      "Loss from 220 to 240 step 6.004295539855957\n",
      "Loss from 240 to 260 step 5.981633973121643\n",
      "Loss from 260 to 280 step 5.970033144950866\n",
      "Loss from 280 to 300 step 6.015690159797669\n",
      "Loss from 300 to 320 step 5.854319095611572\n",
      "Loss from 320 to 340 step 6.023437714576721\n",
      "Loss from 340 to 360 step 5.953654384613037\n",
      "Loss from 360 to 380 step 5.883006310462951\n",
      "Loss from 380 to 400 step 5.7898475408554075\n",
      "Loss from 400 to 420 step 5.902785301208496\n",
      "Loss from 420 to 440 step 6.053717994689942\n",
      "Loss from 440 to 460 step 5.850425219535827\n",
      "Loss from 460 to 480 step 5.784029030799866\n",
      "Loss from 480 to 500 step 5.777919268608093\n",
      "Loss from 500 to 520 step 5.745508456230164\n",
      "Loss from 520 to 540 step 5.768618559837341\n",
      "Loss from 540 to 560 step 5.728899335861206\n",
      "Loss from 560 to 580 step 5.710562467575073\n",
      "Loss from 580 to 600 step 5.779856944084168\n",
      "Loss from 600 to 620 step 5.747948884963989\n",
      "Loss from 620 to 640 step 5.740330147743225\n",
      "Loss from 640 to 660 step 5.657086634635926\n",
      "Loss from 660 to 680 step 5.748719787597656\n",
      "Loss from 680 to 700 step 5.637976789474488\n",
      "Loss from 700 to 720 step 5.590312600135803\n",
      "Loss from 720 to 740 step 5.799010801315307\n",
      "Loss from 740 to 760 step 5.575883531570435\n",
      "Loss from 760 to 780 step 5.628402519226074\n",
      "Loss from 780 to 800 step 5.537129378318786\n",
      "Loss from 800 to 820 step 5.540463399887085\n",
      "Loss from 820 to 840 step 5.6021909475326535\n",
      "Loss from 840 to 860 step 5.7068095922470095\n",
      "Loss from 860 to 880 step 5.568535256385803\n",
      "Loss from 880 to 900 step 5.577781820297242\n",
      "Loss from 900 to 920 step 5.44543423652649\n",
      "Loss from 920 to 940 step 5.5301000595092775\n",
      "Loss from 940 to 960 step 5.521611046791077\n",
      "Loss from 960 to 980 step 5.459099197387696\n",
      "Loss from 980 to 1000 step 5.4561515092849735\n",
      "Loss from 1000 to 1020 step 5.524636340141297\n",
      "Loss from 1020 to 1040 step 5.400701665878296\n",
      "Loss from 1040 to 1060 step 5.485788059234619\n",
      "Loss from 1060 to 1080 step 5.371990752220154\n",
      "Loss from 1080 to 1100 step 5.478455758094787\n",
      "Loss from 1100 to 1120 step 5.428203201293945\n",
      "Loss from 1120 to 1140 step 5.3431655168533325\n",
      "Loss from 1140 to 1160 step 5.36306312084198\n",
      "Loss from 1160 to 1180 step 5.280708503723145\n",
      "Loss from 1180 to 1200 step 5.163995599746704\n",
      "Loss from 1200 to 1220 step 5.1924285888671875\n",
      "Loss from 1220 to 1240 step 5.171817445755005\n",
      "Loss from 1240 to 1260 step 5.153382897377014\n",
      "Loss from 1260 to 1280 step 5.097093105316162\n",
      "Loss from 1280 to 1300 step 5.004420280456543\n",
      "Loss from 1300 to 1320 step 4.915991973876953\n",
      "Loss from 1320 to 1340 step 4.930483508110046\n",
      "Loss from 1340 to 1360 step 4.849213004112244\n",
      "Loss from 1360 to 1380 step 4.693324542045593\n",
      "Loss from 1380 to 1400 step 4.743980503082275\n",
      "Loss from 1400 to 1420 step 4.52218542098999\n",
      "Loss from 1420 to 1440 step 4.416104745864868\n",
      "Loss from 1440 to 1460 step 4.383826160430909\n",
      "Loss from 1460 to 1480 step 4.1994599342346195\n",
      "Loss from 1480 to 1500 step 3.9402536392211913\n",
      "Loss from 1500 to 1520 step 3.8840853929519654\n",
      "Loss from 1520 to 1540 step 3.9154317498207094\n",
      "Loss from 1540 to 1560 step 3.8094978094100953\n",
      "Loss from 1560 to 1580 step 3.6400025367736815\n",
      "Loss from 1580 to 1600 step 3.534165287017822\n",
      "Loss from 1600 to 1620 step 3.421795868873596\n",
      "Loss from 1620 to 1640 step 3.260390043258667\n",
      "Loss from 1640 to 1660 step 3.231971573829651\n",
      "Loss from 1660 to 1680 step 3.077660894393921\n",
      "Loss from 1680 to 1700 step 3.13252329826355\n",
      "Loss from 1700 to 1720 step 2.871336793899536\n",
      "Loss from 1720 to 1740 step 2.769978964328766\n",
      "Loss from 1740 to 1760 step 2.642255687713623\n",
      "Loss from 1760 to 1780 step 2.7300079107284545\n",
      "Loss from 1780 to 1800 step 2.496389937400818\n",
      "Loss from 1800 to 1820 step 2.296148979663849\n",
      "Loss from 1820 to 1840 step 2.2819531559944153\n",
      "Loss from 1840 to 1860 step 2.276650905609131\n",
      "Loss from 1860 to 1880 step 2.091975009441376\n",
      "Loss from 1880 to 1900 step 1.9955237269401551\n",
      "Loss from 1900 to 1920 step 2.1191526472568514\n",
      "Loss from 1920 to 1940 step 2.2592972695827482\n",
      "Loss from 1940 to 1960 step 1.853204995393753\n",
      "Loss from 1960 to 1980 step 1.861301600933075\n",
      "Loss from 1980 to 2000 step 1.8236428201198578\n",
      "Loss from 2000 to 2020 step 1.8218588173389434\n",
      "Loss from 2020 to 2040 step 1.82161545753479\n",
      "Loss from 2040 to 2060 step 1.7210864543914794\n",
      "Loss from 2060 to 2080 step 1.621324360370636\n",
      "Loss from 2080 to 2100 step 1.638492751121521\n",
      "Loss from 2100 to 2120 step 1.5298876404762267\n",
      "Loss from 2120 to 2140 step 1.5578260242938995\n",
      "Loss from 2140 to 2160 step 1.5524632036685944\n",
      "Loss from 2160 to 2180 step 1.6750307619571685\n",
      "Loss from 2180 to 2200 step 1.4552077412605287\n",
      "Loss from 2200 to 2220 step 1.3322960257530212\n",
      "Loss from 2220 to 2240 step 1.3189599514007568\n",
      "Loss from 2240 to 2260 step 1.835617470741272\n",
      "Loss from 2260 to 2280 step 1.3971025407314301\n",
      "Loss from 2280 to 2300 step 1.3134375512599945\n",
      "Loss from 2300 to 2320 step 1.263003671169281\n",
      "Loss from 2320 to 2340 step 1.3625047862529756\n",
      "Loss from 2340 to 2360 step 1.2307153165340423\n",
      "Loss from 2360 to 2380 step 1.1714086771011352\n",
      "Loss from 2380 to 2400 step 1.1890690237283708\n",
      "Loss from 2400 to 2420 step 1.204434484243393\n",
      "Loss from 2420 to 2440 step 1.1163836568593979\n",
      "Loss from 2440 to 2460 step 1.1071819514036179\n",
      "Loss from 2460 to 2480 step 1.1082014471292496\n",
      "Loss from 2480 to 2500 step 1.0112391382455825\n",
      "Loss from 2500 to 2520 step 1.028016310930252\n",
      "Loss from 2520 to 2540 step 1.0724958926439285\n",
      "Loss from 2540 to 2560 step 1.0083614856004715\n",
      "Loss from 2560 to 2580 step 1.0041953712701797\n",
      "Loss from 2580 to 2600 step 1.189646801352501\n",
      "Loss from 2600 to 2620 step 0.9817329823970795\n",
      "Loss from 2620 to 2640 step 0.9952843666076661\n",
      "Loss from 2640 to 2660 step 0.9494968533515931\n",
      "Loss from 2660 to 2680 step 0.9210259407758713\n",
      "Loss from 2680 to 2700 step 0.9419255316257477\n",
      "Loss from 2700 to 2720 step 0.8375903278589248\n",
      "Loss from 2720 to 2740 step 1.2339900314807892\n",
      "Loss from 2740 to 2760 step 1.1930993288755416\n",
      "Loss from 2760 to 2780 step 0.8429695665836334\n",
      "Loss from 2780 to 2800 step 0.8597982704639435\n",
      "Loss from 2800 to 2820 step 1.103814297914505\n",
      "Loss from 2820 to 2840 step 0.9468812763690948\n",
      "Loss from 2840 to 2860 step 0.8890670269727707\n",
      "Loss from 2860 to 2880 step 0.8335377097129821\n",
      "Loss from 2880 to 2900 step 0.7795121967792511\n",
      "Loss from 2900 to 2920 step 0.7945334255695343\n",
      "Loss from 2920 to 2940 step 0.9510454565286637\n",
      "Loss from 2940 to 2960 step 0.815610533952713\n",
      "Loss from 2960 to 2980 step 0.7574942409992218\n",
      "Loss from 2980 to 3000 step 0.8880535721778869\n",
      "Loss from 3000 to 3020 step 0.9531773269176483\n",
      "Loss from 3020 to 3040 step 0.7402178317308425\n",
      "Loss from 3040 to 3060 step 0.8087656408548355\n",
      "Loss from 3060 to 3080 step 0.9577247023582458\n",
      "Loss from 3080 to 3100 step 0.7930222749710083\n",
      "Loss from 3100 to 3120 step 0.6979197233915329\n",
      "Loss from 3120 to 3140 step 0.7452476471662521\n",
      "Loss from 3140 to 3160 step 0.8414967328310012\n",
      "Loss from 3160 to 3180 step 0.6766374856233597\n",
      "Loss from 3180 to 3200 step 0.6840906083583832\n",
      "Loss from 3200 to 3220 step 0.6743333160877227\n",
      "Loss from 3220 to 3240 step 0.740868204832077\n",
      "Loss from 3240 to 3260 step 0.6366116210818291\n",
      "Loss from 3260 to 3280 step 0.7104885846376419\n",
      "Loss from 3280 to 3300 step 0.6910870939493179\n",
      "Loss from 3300 to 3320 step 0.7258759260177612\n",
      "Loss from 3320 to 3340 step 0.6586170554161072\n",
      "Loss from 3340 to 3360 step 0.6691146224737168\n",
      "Loss from 3360 to 3380 step 0.6477464228868485\n",
      "Loss from 3380 to 3400 step 0.6532213151454925\n",
      "Loss from 3400 to 3420 step 0.6147306293249131\n",
      "Loss from 3420 to 3440 step 0.6773879379034042\n",
      "Loss from 3440 to 3460 step 0.6611834228038788\n",
      "Loss from 3460 to 3480 step 0.6575337439775467\n",
      "Loss from 3480 to 3500 step 0.6402526557445526\n",
      "Loss from 3500 to 3520 step 0.5987735286355018\n",
      "Loss from 3520 to 3540 step 0.5816628053784371\n",
      "Loss from 3540 to 3560 step 0.6446446016430855\n",
      "Loss from 3560 to 3580 step 0.7362007170915603\n",
      "Loss from 3580 to 3600 step 0.6435973286628723\n",
      "Loss from 3600 to 3620 step 0.6600595265626907\n",
      "Loss from 3620 to 3640 step 0.5856765612959862\n",
      "Loss from 3640 to 3660 step 0.5690220907330513\n",
      "Loss from 3660 to 3680 step 0.5955666437745094\n",
      "Loss from 3680 to 3700 step 0.6181611895561219\n",
      "Loss from 3700 to 3720 step 0.5658980369567871\n",
      "Loss from 3720 to 3740 step 0.5778284639120101\n",
      "Loss from 3740 to 3760 step 0.5617555022239685\n",
      "Loss from 3760 to 3780 step 0.6140957355499268\n",
      "Loss from 3780 to 3800 step 0.6118627205491066\n",
      "Loss from 3800 to 3820 step 0.5960416316986084\n",
      "Loss from 3820 to 3840 step 0.5925648123025894\n",
      "Loss from 3840 to 3860 step 0.5672468647360802\n",
      "Loss from 3860 to 3880 step 0.5625330924987793\n",
      "Loss from 3880 to 3900 step 0.5737949505448341\n",
      "Loss from 3900 to 3920 step 0.536507461965084\n",
      "Loss from 3920 to 3940 step 0.5527231454849243\n",
      "Loss from 3940 to 3960 step 0.5381853863596916\n",
      "Loss from 3960 to 3980 step 0.5143305480480194\n",
      "Loss from 3980 to 4000 step 0.6231212481856346\n",
      "Loss from 4000 to 4020 step 0.6426347881555557\n",
      "Loss from 4020 to 4040 step 0.5269819796085358\n",
      "Loss from 4040 to 4060 step 0.5181327626109123\n",
      "Loss from 4060 to 4080 step 0.834241347014904\n",
      "Loss from 4080 to 4100 step 0.5761988148093223\n",
      "Loss from 4100 to 4120 step 0.5200384363532067\n",
      "Loss from 4120 to 4140 step 0.4927341595292091\n",
      "Loss from 4140 to 4160 step 0.52872284501791\n",
      "Loss from 4160 to 4180 step 0.5323418438434601\n",
      "Loss from 4180 to 4200 step 0.5886532455682755\n",
      "Loss from 4200 to 4220 step 0.5284082502126694\n",
      "Loss from 4220 to 4240 step 0.5100426822900772\n",
      "Loss from 4240 to 4260 step 0.5072902143001556\n",
      "Loss from 4260 to 4280 step 0.6727320924401283\n",
      "Loss from 4280 to 4300 step 0.5084273651242256\n",
      "Loss from 4300 to 4320 step 0.5122784614562989\n",
      "Loss from 4320 to 4340 step 0.5052503153681756\n",
      "Loss from 4340 to 4360 step 0.4699154645204544\n",
      "Loss from 4360 to 4380 step 0.4771574944257736\n",
      "Loss from 4380 to 4400 step 0.6209059730172157\n",
      "Loss from 4400 to 4420 step 0.5341845840215683\n",
      "Loss from 4420 to 4440 step 0.48367309421300886\n",
      "Loss from 4440 to 4460 step 0.522252906858921\n",
      "Loss from 4460 to 4480 step 0.5545788556337357\n",
      "Loss from 4480 to 4500 step 0.5370993331074715\n",
      "Loss from 4500 to 4520 step 0.5171456143260003\n",
      "Loss from 4520 to 4540 step 0.4617060154676437\n",
      "Loss from 4540 to 4560 step 0.49019824117422106\n",
      "Loss from 4560 to 4580 step 0.4920708164572716\n",
      "Loss from 4580 to 4600 step 0.4991029858589172\n",
      "Loss from 4600 to 4620 step 0.4873970687389374\n",
      "Loss from 4620 to 4640 step 0.5014733165502548\n",
      "Loss from 4640 to 4660 step 0.5174943581223488\n",
      "Loss from 4660 to 4680 step 0.6011943683028221\n",
      "Loss from 4680 to 4700 step 0.5600235760211945\n",
      "Loss from 4700 to 4720 step 0.46315286308526993\n",
      "Loss from 4720 to 4740 step 0.43929972797632216\n",
      "Loss from 4740 to 4760 step 0.6197094216942787\n",
      "Loss from 4760 to 4780 step 0.5775299727916717\n",
      "Loss from 4780 to 4800 step 0.5644958689808846\n",
      "Loss from 4800 to 4820 step 0.4926730811595917\n",
      "Loss from 4820 to 4840 step 0.4993459716439247\n",
      "Loss from 4840 to 4860 step 0.7034438759088516\n",
      "Loss from 4860 to 4880 step 0.5440171897411347\n",
      "Loss from 4880 to 4900 step 0.4480827167630196\n",
      "Loss from 4900 to 4920 step 0.4568950653076172\n",
      "Loss from 4920 to 4940 step 0.5199757546186448\n",
      "Loss from 4940 to 4960 step 0.5425947561860085\n",
      "Loss from 4960 to 4980 step 0.531097038090229\n",
      "Loss from 4980 to 5000 step 0.5787845224142074\n",
      "Loss from 5000 to 5020 step 0.49738929122686387\n",
      "Loss from 5020 to 5040 step 0.43631468564271925\n",
      "Loss from 5040 to 5060 step 0.45028910487890245\n",
      "Loss from 5060 to 5080 step 0.5039397403597832\n",
      "Loss from 5080 to 5100 step 0.44175527691841127\n",
      "Loss from 5100 to 5120 step 0.4997547447681427\n",
      "Loss from 5120 to 5140 step 0.548082922399044\n",
      "Loss from 5140 to 5160 step 0.4512968897819519\n",
      "Loss from 5160 to 5180 step 0.4740728557109833\n",
      "Loss from 5180 to 5200 step 0.4553373321890831\n",
      "Loss from 5200 to 5220 step 0.4370995357632637\n",
      "Loss from 5220 to 5240 step 0.4532883375883102\n",
      "Loss from 5240 to 5260 step 0.605009637773037\n",
      "Loss from 5260 to 5280 step 0.48750378787517545\n",
      "Loss from 5280 to 5300 step 0.44272483587265016\n",
      "Loss from 5300 to 5320 step 0.44645492285490035\n",
      "Loss from 5320 to 5340 step 0.44179692715406416\n",
      "Loss from 5340 to 5360 step 0.5451875671744346\n",
      "Loss from 5360 to 5380 step 0.5289884239435196\n",
      "Loss from 5380 to 5400 step 0.45705423653125765\n",
      "Loss from 5400 to 5420 step 0.4630494207143784\n",
      "Loss from 5420 to 5440 step 0.44573318064212797\n",
      "Loss from 5440 to 5460 step 0.4427622422575951\n",
      "Loss from 5460 to 5480 step 0.4497279331088066\n",
      "Loss from 5480 to 5500 step 0.4533015087246895\n",
      "Loss from 5500 to 5520 step 0.505736568570137\n",
      "Loss from 5520 to 5540 step 0.44158184677362444\n",
      "Loss from 5540 to 5560 step 0.4301795303821564\n",
      "Loss from 5560 to 5580 step 0.4475990816950798\n",
      "Loss from 5580 to 5600 step 0.49156080186367035\n",
      "Loss from 5600 to 5620 step 0.4453552201390266\n",
      "Loss from 5620 to 5640 step 0.4584686875343323\n",
      "Loss from 5640 to 5660 step 0.4475307196378708\n",
      "Loss from 5660 to 5680 step 0.4492894232273102\n",
      "Loss from 5680 to 5700 step 0.4501872450113297\n",
      "Loss from 5700 to 5720 step 0.4355365827679634\n",
      "Loss from 5720 to 5740 step 0.43154419809579847\n",
      "Loss from 5740 to 5760 step 0.5377474576234818\n",
      "Loss from 5760 to 5780 step 0.432750740647316\n",
      "Loss from 5780 to 5800 step 0.4656493663787842\n",
      "Loss from 5800 to 5820 step 0.449885667860508\n",
      "Loss from 5820 to 5840 step 0.4733660027384758\n",
      "Loss from 5840 to 5860 step 0.7197715848684311\n",
      "Loss from 5860 to 5880 step 0.47723750174045565\n",
      "Loss from 5880 to 5900 step 0.4302796319127083\n",
      "Loss from 5900 to 5920 step 0.5884973898530006\n",
      "Loss from 5920 to 5940 step 0.47622784078121183\n",
      "Loss from 5940 to 5960 step 0.44779268503189085\n",
      "Loss from 5960 to 5980 step 0.41780795603990556\n",
      "Loss from 5980 to 6000 step 0.4535822719335556\n",
      "Loss from 6000 to 6020 step 0.4703257232904434\n",
      "Loss from 6020 to 6040 step 0.42144437730312345\n",
      "Loss from 6040 to 6060 step 0.4640024706721306\n",
      "Loss from 6060 to 6080 step 0.41817928552627565\n",
      "Loss from 6080 to 6100 step 0.44758796095848086\n",
      "Loss from 6100 to 6120 step 0.44323769509792327\n",
      "Loss from 6120 to 6140 step 0.4216421529650688\n",
      "Loss from 6140 to 6160 step 0.41525633782148363\n",
      "Loss from 6160 to 6180 step 0.4321424126625061\n",
      "Loss from 6180 to 6200 step 0.4156753093004227\n",
      "Loss from 6200 to 6220 step 0.4680506706237793\n",
      "Loss from 6220 to 6240 step 0.46437172293663026\n",
      "Loss from 6240 to 6260 step 0.4651928901672363\n",
      "Loss from 6260 to 6280 step 0.5836580902338028\n",
      "Loss from 6280 to 6300 step 0.43511743694543836\n",
      "Loss from 6300 to 6320 step 0.4353578418493271\n",
      "Loss from 6320 to 6340 step 0.46714377850294114\n",
      "Loss from 6340 to 6360 step 0.41979639530181884\n",
      "Loss from 6360 to 6380 step 0.4421740472316742\n",
      "Loss from 6380 to 6400 step 0.42733375281095504\n",
      "Loss from 6400 to 6420 step 0.4086941510438919\n",
      "Loss from 6420 to 6440 step 0.4121830925345421\n",
      "Loss from 6440 to 6460 step 0.4173476532101631\n",
      "Loss from 6460 to 6480 step 0.4934186488389969\n",
      "Loss from 6480 to 6500 step 0.42096049934625623\n",
      "Loss from 6500 to 6520 step 0.4444461092352867\n",
      "Loss from 6520 to 6540 step 0.4134635403752327\n",
      "Loss from 6540 to 6560 step 0.4122330069541931\n",
      "Loss from 6560 to 6580 step 0.43542168438434603\n",
      "Loss from 6580 to 6600 step 0.41431119441986086\n",
      "Loss from 6600 to 6620 step 0.4553636580705643\n",
      "Loss from 6620 to 6640 step 0.4072923377156258\n",
      "Loss from 6640 to 6660 step 0.4445396766066551\n",
      "Loss from 6660 to 6680 step 0.43119450509548185\n",
      "Loss from 6680 to 6700 step 0.4121833562850952\n",
      "Loss from 6700 to 6720 step 0.4281520664691925\n",
      "Loss from 6720 to 6740 step 0.41697459369897844\n",
      "Loss from 6740 to 6760 step 0.3866744801402092\n",
      "Loss from 6760 to 6780 step 0.4266080468893051\n",
      "Loss from 6780 to 6800 step 0.45498381555080414\n",
      "Loss from 6800 to 6820 step 0.43581755459308624\n",
      "Loss from 6820 to 6840 step 0.4286061733961105\n",
      "Loss from 6840 to 6860 step 0.40583430528640746\n",
      "Loss from 6860 to 6880 step 0.42981537729501723\n",
      "Loss from 6880 to 6900 step 0.47315243929624556\n",
      "Loss from 6900 to 6920 step 0.45816559046506883\n",
      "Loss from 6920 to 6940 step 0.41190360188484193\n",
      "Loss from 6940 to 6960 step 0.4147530049085617\n",
      "Loss from 6960 to 6980 step 0.39814116060733795\n",
      "Loss from 6980 to 7000 step 0.3912652626633644\n",
      "Loss from 7000 to 7020 step 0.4177718386054039\n",
      "Loss from 7020 to 7040 step 0.41104515343904496\n",
      "Loss from 7040 to 7060 step 0.4203516557812691\n",
      "Loss from 7060 to 7080 step 0.4089223250746727\n",
      "Loss from 7080 to 7100 step 0.42879257500171664\n",
      "Loss from 7100 to 7120 step 0.40219583064317704\n",
      "Loss from 7120 to 7140 step 0.3987698554992676\n",
      "Loss from 7140 to 7160 step 0.4265479952096939\n",
      "Loss from 7160 to 7180 step 0.4131385341286659\n",
      "Loss from 7180 to 7200 step 0.39904517233371734\n",
      "Loss from 7200 to 7220 step 0.4103184476494789\n",
      "Loss from 7220 to 7240 step 0.41426748037338257\n",
      "Loss from 7240 to 7260 step 0.453546068072319\n",
      "Loss from 7260 to 7280 step 0.4261824667453766\n",
      "Loss from 7280 to 7300 step 0.39475787431001663\n",
      "Loss from 7300 to 7320 step 0.3964499652385712\n",
      "Loss from 7320 to 7340 step 0.4919922575354576\n",
      "Loss from 7340 to 7360 step 0.4141779586672783\n",
      "Loss from 7360 to 7380 step 0.5828270673751831\n",
      "Loss from 7380 to 7400 step 0.4288134425878525\n",
      "Loss from 7400 to 7420 step 0.38344122022390364\n",
      "Loss from 7420 to 7440 step 0.39194679707288743\n",
      "Loss from 7440 to 7460 step 0.41608839482069016\n",
      "Loss from 7460 to 7480 step 0.3979507595300674\n",
      "Loss from 7480 to 7500 step 0.48874045014381406\n",
      "Loss from 7500 to 7520 step 0.42240539491176604\n",
      "Loss from 7520 to 7540 step 0.3880756586790085\n",
      "Loss from 7540 to 7560 step 0.41733008325099946\n",
      "Loss from 7560 to 7580 step 0.41838966608047484\n",
      "Loss from 7580 to 7600 step 0.4446929797530174\n",
      "Loss from 7600 to 7620 step 0.44124011397361756\n",
      "Loss from 7620 to 7640 step 0.423183935880661\n",
      "Loss from 7640 to 7660 step 0.39907764494419096\n",
      "Loss from 7660 to 7680 step 0.4017415702342987\n",
      "Loss from 7680 to 7700 step 0.39778531789779664\n",
      "Loss from 7700 to 7720 step 0.4365675956010818\n",
      "Loss from 7720 to 7740 step 0.5494081184267998\n",
      "Loss from 7740 to 7760 step 0.42703473567962646\n",
      "Loss from 7760 to 7780 step 0.37661223858594894\n",
      "Loss from 7780 to 7800 step 0.39882829040288925\n",
      "Loss from 7800 to 7820 step 0.4088380828499794\n",
      "Loss from 7820 to 7840 step 0.40936566442251204\n",
      "Loss from 7840 to 7860 step 0.38683379888534547\n",
      "Loss from 7860 to 7880 step 0.3893313527107239\n",
      "Loss from 7880 to 7900 step 0.3947789490222931\n",
      "Loss from 7900 to 7920 step 0.4599944815039635\n",
      "Loss from 7920 to 7940 step 0.4174868166446686\n",
      "Loss from 7940 to 7960 step 0.38886083364486695\n",
      "Loss from 7960 to 7980 step 0.4466474950313568\n",
      "Loss from 7980 to 8000 step 0.4111865684390068\n",
      "Loss from 8000 to 8020 step 0.42530961483716967\n",
      "Loss from 8020 to 8040 step 0.4089673399925232\n",
      "Loss from 8040 to 8060 step 0.41004966795444486\n",
      "Loss from 8060 to 8080 step 0.4284537315368652\n",
      "Loss from 8080 to 8100 step 0.38656324446201323\n",
      "Loss from 8100 to 8120 step 0.39630320817232134\n",
      "Loss from 8120 to 8140 step 0.40865263640880584\n",
      "Loss from 8140 to 8160 step 0.40189359784126283\n",
      "Loss from 8160 to 8180 step 0.47455762773752214\n",
      "Loss from 8180 to 8200 step 0.41044238954782486\n",
      "Loss from 8200 to 8220 step 0.39698383063077924\n",
      "Loss from 8220 to 8240 step 0.3922839045524597\n",
      "Loss from 8240 to 8260 step 0.41966637820005415\n",
      "Loss from 8260 to 8280 step 0.5068163380026818\n",
      "Loss from 8280 to 8300 step 0.43383061438798903\n",
      "Loss from 8300 to 8320 step 0.38363858461380007\n",
      "Loss from 8320 to 8340 step 0.3804789468646049\n",
      "Loss from 8340 to 8360 step 0.4276599586009979\n",
      "Loss from 8360 to 8380 step 0.3979118809103966\n",
      "Loss from 8380 to 8400 step 0.39159719944000243\n",
      "Loss from 8400 to 8420 step 0.5028366297483444\n",
      "Loss from 8420 to 8440 step 0.38561523109674456\n",
      "Loss from 8440 to 8460 step 0.4177102103829384\n",
      "Loss from 8460 to 8480 step 0.39158805757761\n",
      "Loss from 8480 to 8500 step 0.3836270272731781\n",
      "Loss from 8500 to 8520 step 0.444809028506279\n",
      "Loss from 8520 to 8540 step 0.502854335308075\n",
      "Loss from 8540 to 8560 step 0.3924135029315948\n",
      "Loss from 8560 to 8580 step 0.37361405193805697\n",
      "Loss from 8580 to 8600 step 0.36899677813053133\n",
      "Loss from 8600 to 8620 step 0.41100613623857496\n",
      "Loss from 8620 to 8640 step 0.4519548431038857\n",
      "Loss from 8640 to 8660 step 0.39385790973901746\n",
      "Loss from 8660 to 8680 step 0.40269952863454817\n",
      "Loss from 8680 to 8700 step 0.4185687407851219\n",
      "Loss from 8700 to 8720 step 0.3877963811159134\n",
      "Loss from 8720 to 8740 step 0.38590996265411376\n",
      "Loss from 8740 to 8760 step 0.37260761857032776\n",
      "Loss from 8760 to 8780 step 0.3793163150548935\n",
      "Loss from 8780 to 8800 step 0.382142947614193\n",
      "Loss from 8800 to 8820 step 0.3819702506065369\n",
      "Loss from 8820 to 8840 step 0.40105228275060656\n",
      "Loss from 8840 to 8860 step 0.39856479614973067\n",
      "Loss from 8860 to 8880 step 0.3775878041982651\n",
      "Loss from 8880 to 8900 step 0.3996960401535034\n",
      "Loss from 8900 to 8920 step 0.3957841321825981\n",
      "Loss from 8920 to 8940 step 0.37708383798599243\n",
      "Loss from 8940 to 8960 step 0.401773127913475\n",
      "Loss from 8960 to 8980 step 0.3844050467014313\n",
      "Loss from 8980 to 9000 step 0.41305204331874845\n",
      "Loss from 9000 to 9020 step 0.3954710990190506\n",
      "Loss from 9020 to 9040 step 0.4135809287428856\n",
      "Loss from 9040 to 9060 step 0.41099008172750473\n",
      "Loss from 9060 to 9080 step 0.4076341077685356\n",
      "Loss from 9080 to 9100 step 0.38238169997930527\n",
      "Loss from 9100 to 9120 step 0.3937489852309227\n",
      "Loss from 9120 to 9140 step 0.38727476000785827\n",
      "Loss from 9140 to 9160 step 0.391971355676651\n",
      "Loss from 9160 to 9180 step 0.3854297652840614\n",
      "Loss from 9180 to 9200 step 0.3942541927099228\n",
      "Loss from 9200 to 9220 step 0.3768860474228859\n",
      "Loss from 9220 to 9240 step 0.39041696935892106\n",
      "Loss from 9240 to 9260 step 0.4903286099433899\n",
      "Loss from 9260 to 9280 step 0.38947402089834215\n",
      "Loss from 9280 to 9300 step 0.3749333471059799\n",
      "Loss from 9300 to 9320 step 0.4275518760085106\n",
      "Loss from 9320 to 9340 step 0.3933549106121063\n",
      "Loss from 9340 to 9360 step 0.3721606060862541\n",
      "Loss from 9360 to 9380 step 0.3654185369610786\n",
      "Loss from 9380 to 9400 step 0.3874223932623863\n",
      "Loss from 9400 to 9420 step 0.40812998712062837\n",
      "Loss from 9420 to 9440 step 0.3916334867477417\n",
      "Loss from 9440 to 9460 step 0.3882841721177101\n",
      "Loss from 9460 to 9480 step 0.38371817469596864\n",
      "Loss from 9480 to 9500 step 0.3840354233980179\n",
      "Loss from 9500 to 9520 step 0.3778324589133263\n",
      "Loss from 9520 to 9540 step 0.38467832654714584\n",
      "Loss from 9540 to 9560 step 0.40758263021707536\n",
      "Loss from 9560 to 9580 step 0.3579834908246994\n",
      "Loss from 9580 to 9600 step 0.3668885201215744\n",
      "Loss from 9600 to 9620 step 0.3837777227163315\n",
      "Loss from 9620 to 9640 step 0.37332554012537\n",
      "Loss from 9640 to 9660 step 0.39847404360771177\n",
      "Loss from 9660 to 9680 step 0.38485843390226365\n",
      "Loss from 9680 to 9700 step 0.4352338045835495\n",
      "Loss from 9700 to 9720 step 0.37618850320577624\n",
      "Loss from 9720 to 9740 step 0.3789780631661415\n",
      "Loss from 9740 to 9760 step 0.39252941608428954\n",
      "Loss from 9760 to 9780 step 0.37223539799451827\n",
      "Loss from 9780 to 9800 step 0.3923075035214424\n",
      "Loss from 9800 to 9820 step 0.37274367213249204\n",
      "Loss from 9820 to 9840 step 0.4347687691450119\n",
      "Loss from 9840 to 9860 step 0.5054438307881355\n",
      "Loss from 9860 to 9880 step 0.3863694205880165\n",
      "Loss from 9880 to 9900 step 0.376750984787941\n",
      "Loss from 9900 to 9920 step 0.36088770627975464\n",
      "Loss from 9920 to 9940 step 0.37911845594644544\n",
      "Loss from 9940 to 9960 step 0.3877283900976181\n",
      "Loss from 9960 to 9980 step 0.375961622595787\n",
      "Loss from 9980 to 10000 step 0.38467767387628554\n",
      "Loss from 10000 to 10020 step 0.356402263045311\n",
      "Loss from 10020 to 10040 step 0.36470907330513\n",
      "Loss from 10040 to 10060 step 0.47334693372249603\n",
      "Loss from 10060 to 10080 step 0.40358795672655107\n",
      "Loss from 10080 to 10100 step 0.47431349009275436\n",
      "Loss from 10100 to 10120 step 0.3805006563663483\n",
      "Loss from 10120 to 10140 step 0.39515587836503985\n",
      "Loss from 10140 to 10160 step 0.35536395460367204\n",
      "Loss from 10160 to 10180 step 0.38069520741701124\n",
      "Loss from 10180 to 10200 step 0.4085976704955101\n",
      "Loss from 10200 to 10220 step 0.3954425871372223\n",
      "Loss from 10220 to 10240 step 0.41209695041179656\n",
      "Loss from 10240 to 10260 step 0.3765098720788956\n",
      "Loss from 10260 to 10280 step 0.41304075717926025\n",
      "Loss from 10280 to 10300 step 0.41323941946029663\n",
      "Loss from 10300 to 10320 step 0.3689355581998825\n",
      "Loss from 10320 to 10340 step 0.3738356441259384\n",
      "Loss from 10340 to 10360 step 0.37957875728607177\n",
      "Loss from 10360 to 10380 step 0.3445402517914772\n",
      "Loss from 10380 to 10400 step 0.3658858388662338\n",
      "Loss from 10400 to 10420 step 0.34496581107378005\n",
      "Loss from 10420 to 10440 step 0.3749757096171379\n",
      "Loss from 10440 to 10460 step 0.37433150857687\n",
      "Loss from 10460 to 10480 step 0.39565335363149645\n",
      "Loss from 10480 to 10500 step 0.398822084069252\n",
      "Loss from 10500 to 10520 step 0.3610728710889816\n",
      "Loss from 10520 to 10540 step 0.36212461441755295\n",
      "Loss from 10540 to 10560 step 0.3796705186367035\n",
      "Loss from 10560 to 10580 step 0.34664351791143416\n",
      "Loss from 10580 to 10600 step 0.37943138033151624\n",
      "Loss from 10600 to 10620 step 0.42042262107133865\n",
      "Loss from 10620 to 10640 step 0.3799171656370163\n",
      "Loss from 10640 to 10660 step 0.3485451936721802\n",
      "Loss from 10660 to 10680 step 0.34107470214366914\n",
      "Loss from 10680 to 10700 step 0.4133499562740326\n",
      "Loss from 10700 to 10720 step 0.3663335397839546\n",
      "Loss from 10720 to 10740 step 0.349331496655941\n",
      "Loss from 10740 to 10760 step 0.37514118701219556\n",
      "Loss from 10760 to 10780 step 0.38350452333688734\n",
      "Loss from 10780 to 10800 step 0.3959411188960075\n",
      "Loss from 10800 to 10820 step 0.3737228125333786\n",
      "Loss from 10820 to 10840 step 0.3929137647151947\n",
      "Loss from 10840 to 10860 step 0.34841516315937043\n",
      "Loss from 10860 to 10880 step 0.4073489770293236\n",
      "Loss from 10880 to 10900 step 0.41593694388866426\n",
      "Loss from 10900 to 10920 step 0.37561787366867067\n",
      "Loss from 10920 to 10940 step 0.35276919305324556\n",
      "Loss from 10940 to 10960 step 0.35657077729701997\n",
      "Loss from 10960 to 10980 step 0.3545414164662361\n",
      "Loss from 10980 to 11000 step 0.3849971488118172\n",
      "Loss from 11000 to 11020 step 0.39144736230373384\n",
      "Loss from 11020 to 11040 step 0.37113807797431947\n",
      "Loss from 11040 to 11060 step 0.36513982713222504\n",
      "Loss from 11060 to 11080 step 0.37279233932495115\n",
      "Loss from 11080 to 11100 step 0.35073166638612746\n",
      "Loss from 11100 to 11120 step 0.366599814593792\n",
      "Loss from 11120 to 11140 step 0.3773523837327957\n",
      "Loss from 11140 to 11160 step 0.34305770993232726\n",
      "Loss from 11160 to 11180 step 0.3755544751882553\n",
      "Loss from 11180 to 11200 step 0.3514241844415665\n",
      "Loss from 11200 to 11220 step 0.3502845838665962\n",
      "Loss from 11220 to 11240 step 0.36492629051208497\n",
      "Loss from 11240 to 11260 step 0.37449924647808075\n",
      "Loss from 11260 to 11280 step 0.3818128928542137\n",
      "Loss from 11280 to 11300 step 0.3577523246407509\n",
      "Loss from 11300 to 11320 step 0.35738610476255417\n",
      "Loss from 11320 to 11340 step 0.34937907457351686\n",
      "Loss from 11340 to 11360 step 0.3905605524778366\n",
      "Loss from 11360 to 11380 step 0.3927771806716919\n",
      "Loss from 11380 to 11400 step 0.3828701630234718\n",
      "Loss from 11400 to 11420 step 0.35964909195899963\n",
      "Loss from 11420 to 11440 step 0.359689898788929\n",
      "Loss from 11440 to 11460 step 0.35752061307430266\n",
      "Loss from 11460 to 11480 step 0.35201561003923415\n",
      "Loss from 11480 to 11500 step 0.3506480574607849\n",
      "Loss from 11500 to 11520 step 0.4140655666589737\n",
      "Loss from 11520 to 11540 step 0.3414875760674477\n",
      "Loss from 11540 to 11560 step 0.3813727587461472\n",
      "Loss from 11560 to 11580 step 0.3584734693169594\n",
      "Loss from 11580 to 11600 step 0.3895892098546028\n",
      "Loss from 11600 to 11620 step 0.3528212413191795\n",
      "Loss from 11620 to 11640 step 0.34458575695753096\n",
      "Loss from 11640 to 11660 step 0.3439068868756294\n",
      "Loss from 11660 to 11680 step 0.3723790839314461\n",
      "Loss from 11680 to 11700 step 0.3400786116719246\n",
      "Loss from 11700 to 11720 step 0.628485769033432\n",
      "Loss from 11720 to 11740 step 0.3745870664715767\n",
      "Loss from 11740 to 11760 step 0.3549761205911636\n",
      "Loss from 11760 to 11780 step 0.3645480126142502\n",
      "Loss from 11780 to 11800 step 0.35316399186849595\n",
      "Loss from 11800 to 11820 step 0.38350636810064315\n",
      "Loss from 11820 to 11840 step 0.3914844527840614\n",
      "Loss from 11840 to 11860 step 0.38722449243068696\n",
      "Loss from 11860 to 11880 step 0.3540175721049309\n",
      "Loss from 11880 to 11900 step 0.3524864912033081\n",
      "Loss from 11900 to 11920 step 0.37791178077459336\n",
      "Loss from 11920 to 11940 step 0.3871900662779808\n",
      "Loss from 11940 to 11960 step 0.34256900697946546\n",
      "Loss from 11960 to 11980 step 0.34104081243276596\n",
      "Loss from 11980 to 12000 step 0.3882798790931702\n",
      "Loss from 12000 to 12020 step 0.3871953561902046\n",
      "Loss from 12020 to 12040 step 0.36883160918951036\n",
      "Loss from 12040 to 12060 step 0.3420269191265106\n",
      "Loss from 12060 to 12080 step 0.4998260661959648\n",
      "Loss from 12080 to 12100 step 0.3729130491614342\n",
      "Loss from 12100 to 12120 step 0.34031821638345716\n",
      "Loss from 12120 to 12140 step 0.343852761387825\n",
      "Loss from 12140 to 12160 step 0.3543479859828949\n",
      "Loss from 12160 to 12180 step 0.364301872253418\n",
      "Loss from 12180 to 12200 step 0.35190682858228683\n",
      "Loss from 12200 to 12220 step 0.3497376814484596\n",
      "Loss from 12220 to 12240 step 0.33554253578186033\n",
      "Loss from 12240 to 12260 step 0.35586502999067304\n",
      "Loss from 12260 to 12280 step 0.3591646194458008\n",
      "Loss from 12280 to 12300 step 0.3840069442987442\n",
      "Loss from 12300 to 12320 step 0.33810562193393706\n",
      "Loss from 12320 to 12340 step 0.3599383756518364\n",
      "Loss from 12340 to 12360 step 0.3500966250896454\n",
      "Loss from 12360 to 12380 step 0.3598883286118507\n",
      "Loss from 12380 to 12400 step 0.35139979124069215\n",
      "Loss from 12400 to 12420 step 0.3321566179394722\n",
      "Loss from 12420 to 12440 step 0.37040635496377944\n",
      "Loss from 12440 to 12460 step 0.3616487756371498\n",
      "Loss from 12460 to 12480 step 0.3589713364839554\n",
      "Loss from 12480 to 12500 step 0.34515133798122405\n",
      "Train loss: 1.220115881881714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [01:59<00:00, 837.65 examples/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d777aecfd1394d1bbfe9ac4f8ccd5a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss from 0 to 20 step 0.33669422268867494\n",
      "Loss from 20 to 40 step 0.33777383863925936\n",
      "Loss from 40 to 60 step 0.35498461723327634\n",
      "Loss from 60 to 80 step 0.3584600403904915\n",
      "Loss from 80 to 100 step 0.354111148416996\n",
      "Loss from 100 to 120 step 0.3348711460828781\n",
      "Loss from 120 to 140 step 0.33628710359334946\n",
      "Loss from 140 to 160 step 0.3861187607049942\n",
      "Loss from 160 to 180 step 0.36915919184684753\n",
      "Loss from 180 to 200 step 0.3411905601620674\n",
      "Loss from 200 to 220 step 0.3465409651398659\n",
      "Loss from 220 to 240 step 0.34457228034734727\n",
      "Loss from 240 to 260 step 0.33218675702810285\n",
      "Loss from 260 to 280 step 0.33961435556411745\n",
      "Loss from 280 to 300 step 0.3350308120250702\n",
      "Loss from 300 to 320 step 0.3546433448791504\n",
      "Loss from 320 to 340 step 0.3483290195465088\n",
      "Loss from 340 to 360 step 0.34993422478437425\n",
      "Loss from 360 to 380 step 0.44374899864196776\n",
      "Loss from 380 to 400 step 0.3405505523085594\n",
      "Loss from 400 to 420 step 0.32917495667934416\n",
      "Loss from 420 to 440 step 0.35489716827869416\n",
      "Loss from 440 to 460 step 0.33727542161941526\n",
      "Loss from 460 to 480 step 0.34540307372808454\n",
      "Loss from 480 to 500 step 0.4059049665927887\n",
      "Loss from 500 to 520 step 0.35304695516824725\n",
      "Loss from 520 to 540 step 0.3509465232491493\n",
      "Loss from 540 to 560 step 0.33333310335874555\n",
      "Loss from 560 to 580 step 0.3481254205107689\n",
      "Loss from 580 to 600 step 0.35349403470754626\n",
      "Loss from 600 to 620 step 0.35905386656522753\n",
      "Loss from 620 to 640 step 0.3390319526195526\n",
      "Loss from 640 to 660 step 0.33788304924964907\n",
      "Loss from 660 to 680 step 0.3340247243642807\n",
      "Loss from 680 to 700 step 0.3440005794167519\n",
      "Loss from 700 to 720 step 0.3422289565205574\n",
      "Loss from 720 to 740 step 0.3370689630508423\n",
      "Loss from 740 to 760 step 0.3366679444909096\n",
      "Loss from 760 to 780 step 0.3528121650218964\n",
      "Loss from 780 to 800 step 0.3497111961245537\n",
      "Loss from 800 to 820 step 0.3487657025456429\n",
      "Loss from 820 to 840 step 0.35420013666152955\n",
      "Loss from 840 to 860 step 0.35224505364894865\n",
      "Loss from 860 to 880 step 0.3532710328698158\n",
      "Loss from 880 to 900 step 0.33021524995565416\n",
      "Loss from 900 to 920 step 0.34296057373285294\n",
      "Loss from 920 to 940 step 0.3341109067201614\n",
      "Loss from 940 to 960 step 0.3932799130678177\n",
      "Loss from 960 to 980 step 0.35115426778793335\n",
      "Loss from 980 to 1000 step 0.3366148263216019\n",
      "Loss from 1000 to 1020 step 0.34277876019477843\n",
      "Loss from 1020 to 1040 step 0.3722430393099785\n",
      "Loss from 1040 to 1060 step 0.3348841965198517\n",
      "Loss from 1060 to 1080 step 0.34024099409580233\n",
      "Loss from 1080 to 1100 step 0.3480712115764618\n",
      "Loss from 1100 to 1120 step 0.3421962887048721\n",
      "Loss from 1120 to 1140 step 0.3650276124477386\n",
      "Loss from 1140 to 1160 step 0.3464974701404572\n",
      "Loss from 1160 to 1180 step 0.32695759683847425\n",
      "Loss from 1180 to 1200 step 0.3376230984926224\n",
      "Loss from 1200 to 1220 step 0.3760827422142029\n",
      "Loss from 1220 to 1240 step 0.33669008761644365\n",
      "Loss from 1240 to 1260 step 0.33683458864688876\n",
      "Loss from 1260 to 1280 step 0.372811833024025\n",
      "Loss from 1280 to 1300 step 0.38068499565124514\n",
      "Loss from 1300 to 1320 step 0.3441924400627613\n",
      "Loss from 1320 to 1340 step 0.35805346816778183\n",
      "Loss from 1340 to 1360 step 0.34799018055200576\n",
      "Loss from 1360 to 1380 step 0.33305744379758834\n",
      "Loss from 1380 to 1400 step 0.32742692679166796\n",
      "Loss from 1400 to 1420 step 0.3295604959130287\n",
      "Loss from 1420 to 1440 step 0.3427386537194252\n",
      "Loss from 1440 to 1460 step 0.37337699383497236\n",
      "Loss from 1460 to 1480 step 0.3266285374760628\n",
      "Loss from 1480 to 1500 step 0.33327711224555967\n",
      "Loss from 1500 to 1520 step 0.3350709617137909\n",
      "Loss from 1520 to 1540 step 0.3362409174442291\n",
      "Loss from 1540 to 1560 step 0.33970594108104707\n",
      "Loss from 1560 to 1580 step 0.38339787125587466\n",
      "Loss from 1580 to 1600 step 0.3304051548242569\n",
      "Loss from 1600 to 1620 step 0.3322494447231293\n",
      "Loss from 1620 to 1640 step 0.34157567769289016\n",
      "Loss from 1640 to 1660 step 0.32807438522577287\n",
      "Loss from 1660 to 1680 step 0.3195466250181198\n",
      "Loss from 1680 to 1700 step 0.31838337928056715\n",
      "Loss from 1700 to 1720 step 0.3743190407752991\n",
      "Loss from 1720 to 1740 step 0.33005466014146806\n",
      "Loss from 1740 to 1760 step 0.3362256482243538\n",
      "Loss from 1760 to 1780 step 0.3287655860185623\n",
      "Loss from 1780 to 1800 step 0.3823921427130699\n",
      "Loss from 1800 to 1820 step 0.3485300913453102\n",
      "Loss from 1820 to 1840 step 0.35552549064159394\n",
      "Loss from 1840 to 1860 step 0.3677700474858284\n",
      "Loss from 1860 to 1880 step 0.319574648141861\n",
      "Loss from 1880 to 1900 step 0.3320412218570709\n",
      "Loss from 1900 to 1920 step 0.3275682359933853\n",
      "Loss from 1920 to 1940 step 0.329620486497879\n",
      "Loss from 1940 to 1960 step 0.32215360552072525\n",
      "Loss from 1960 to 1980 step 0.3398761510848999\n",
      "Loss from 1980 to 2000 step 0.3360524386167526\n",
      "Loss from 2000 to 2020 step 0.3174732565879822\n",
      "Loss from 2020 to 2040 step 0.35327011048793794\n",
      "Loss from 2040 to 2060 step 0.32603076845407486\n",
      "Loss from 2060 to 2080 step 0.3297982424497604\n",
      "Loss from 2080 to 2100 step 0.32584069669246674\n",
      "Loss from 2100 to 2120 step 0.3397798344492912\n",
      "Loss from 2120 to 2140 step 0.31877670884132386\n",
      "Loss from 2140 to 2160 step 0.3222000390291214\n",
      "Loss from 2160 to 2180 step 0.33225916177034376\n",
      "Loss from 2180 to 2200 step 0.3663985446095467\n",
      "Loss from 2200 to 2220 step 0.3200755432248116\n",
      "Loss from 2220 to 2240 step 0.3627624973654747\n",
      "Loss from 2240 to 2260 step 0.37663440108299256\n",
      "Loss from 2260 to 2280 step 0.413971446454525\n",
      "Loss from 2280 to 2300 step 0.4208473667502403\n",
      "Loss from 2300 to 2320 step 0.31875895112752917\n",
      "Loss from 2320 to 2340 step 0.3366917476058006\n",
      "Loss from 2340 to 2360 step 0.3236491769552231\n",
      "Loss from 2360 to 2380 step 0.3250300198793411\n",
      "Loss from 2380 to 2400 step 0.3233277425169945\n",
      "Loss from 2400 to 2420 step 0.49295934587717055\n",
      "Loss from 2420 to 2440 step 0.34017913043498993\n",
      "Loss from 2440 to 2460 step 0.32199406027793886\n",
      "Loss from 2460 to 2480 step 0.4555960401892662\n",
      "Loss from 2480 to 2500 step 0.33591154515743255\n",
      "Loss from 2500 to 2520 step 0.3260338857769966\n",
      "Loss from 2520 to 2540 step 0.3149242579936981\n",
      "Loss from 2540 to 2560 step 0.3675594046711922\n",
      "Loss from 2560 to 2580 step 0.3171596318483353\n",
      "Loss from 2580 to 2600 step 0.3177425071597099\n",
      "Loss from 2600 to 2620 step 0.31731575131416323\n",
      "Loss from 2620 to 2640 step 0.33021653443574905\n",
      "Loss from 2640 to 2660 step 0.3144179493188858\n",
      "Loss from 2660 to 2680 step 0.3311672300100327\n",
      "Loss from 2680 to 2700 step 0.34191869795322416\n",
      "Loss from 2700 to 2720 step 0.3707868531346321\n",
      "Loss from 2720 to 2740 step 0.3263311877846718\n",
      "Loss from 2740 to 2760 step 0.3348573684692383\n",
      "Loss from 2760 to 2780 step 0.31891111433506014\n",
      "Loss from 2780 to 2800 step 0.3237702906131744\n",
      "Loss from 2800 to 2820 step 0.34442501962184907\n",
      "Loss from 2820 to 2840 step 0.3109879806637764\n",
      "Loss from 2840 to 2860 step 0.32486444264650344\n",
      "Loss from 2860 to 2880 step 0.31186521202325823\n",
      "Loss from 2880 to 2900 step 0.3317306190729141\n",
      "Loss from 2900 to 2920 step 0.31990910321474075\n",
      "Loss from 2920 to 2940 step 0.3229286208748817\n",
      "Loss from 2940 to 2960 step 0.3433599188923836\n",
      "Loss from 2960 to 2980 step 0.31968365162611007\n",
      "Loss from 2980 to 3000 step 0.35040497183799746\n",
      "Loss from 3000 to 3020 step 0.4051141083240509\n",
      "Loss from 3020 to 3040 step 0.3106518656015396\n",
      "Loss from 3040 to 3060 step 0.3134378671646118\n",
      "Loss from 3060 to 3080 step 0.31553331911563876\n",
      "Loss from 3080 to 3100 step 0.31542538553476335\n",
      "Loss from 3100 to 3120 step 0.33605213463306427\n",
      "Loss from 3120 to 3140 step 0.31044245809316634\n",
      "Loss from 3140 to 3160 step 0.32846878170967103\n",
      "Loss from 3160 to 3180 step 0.31803876906633377\n",
      "Loss from 3180 to 3200 step 0.31112888902425767\n",
      "Loss from 3200 to 3220 step 0.3281546920537949\n",
      "Loss from 3220 to 3240 step 0.3253572627902031\n",
      "Loss from 3240 to 3260 step 0.32081461101770403\n",
      "Loss from 3260 to 3280 step 0.33270551115274427\n",
      "Loss from 3280 to 3300 step 0.3345415011048317\n",
      "Loss from 3300 to 3320 step 0.3107934340834618\n",
      "Loss from 3320 to 3340 step 0.32719982862472535\n",
      "Loss from 3340 to 3360 step 0.32694068998098375\n",
      "Loss from 3360 to 3380 step 0.3191279098391533\n",
      "Loss from 3380 to 3400 step 0.32670332193374635\n",
      "Loss from 3400 to 3420 step 0.3612625807523727\n",
      "Loss from 3420 to 3440 step 0.3379530906677246\n",
      "Loss from 3440 to 3460 step 0.3164822101593018\n",
      "Loss from 3460 to 3480 step 0.32571663707494736\n",
      "Loss from 3480 to 3500 step 0.3194228053092957\n",
      "Loss from 3500 to 3520 step 0.3267457842826843\n",
      "Loss from 3520 to 3540 step 0.31970404386520385\n",
      "Loss from 3540 to 3560 step 0.3552929311990738\n",
      "Loss from 3560 to 3580 step 0.3258989155292511\n",
      "Loss from 3580 to 3600 step 0.3272753685712814\n",
      "Loss from 3600 to 3620 step 0.3145360007882118\n",
      "Loss from 3620 to 3640 step 0.32401988506317136\n",
      "Loss from 3640 to 3660 step 0.3369316697120667\n",
      "Loss from 3660 to 3680 step 0.32435416281223295\n",
      "Loss from 3680 to 3700 step 0.3212937921285629\n",
      "Loss from 3700 to 3720 step 0.329822801053524\n",
      "Loss from 3720 to 3740 step 0.31790564954280853\n",
      "Loss from 3740 to 3760 step 0.31068356335163116\n",
      "Loss from 3760 to 3780 step 0.32828068882226946\n",
      "Loss from 3780 to 3800 step 0.31621197909116744\n",
      "Loss from 3800 to 3820 step 0.35244898349046705\n",
      "Loss from 3820 to 3840 step 0.3565119430422783\n",
      "Loss from 3840 to 3860 step 0.34530953466892245\n",
      "Loss from 3860 to 3880 step 0.3240073248744011\n",
      "Loss from 3880 to 3900 step 0.3074388042092323\n",
      "Loss from 3900 to 3920 step 0.3125732272863388\n",
      "Loss from 3920 to 3940 step 0.31996279060840604\n",
      "Loss from 3940 to 3960 step 0.31457254886627195\n",
      "Loss from 3960 to 3980 step 0.32012017369270324\n",
      "Loss from 3980 to 4000 step 0.30995842069387436\n",
      "Loss from 4000 to 4020 step 0.3237838879227638\n",
      "Loss from 4020 to 4040 step 0.32475097477436066\n",
      "Loss from 4040 to 4060 step 0.33491256684064863\n",
      "Loss from 4060 to 4080 step 0.36351718902587893\n",
      "Loss from 4080 to 4100 step 0.38306088745594025\n",
      "Loss from 4100 to 4120 step 0.3204599440097809\n",
      "Loss from 4120 to 4140 step 0.30576241463422776\n",
      "Loss from 4140 to 4160 step 0.30360178351402284\n",
      "Loss from 4160 to 4180 step 0.30676460415124895\n",
      "Loss from 4180 to 4200 step 0.33363051116466524\n",
      "Loss from 4200 to 4220 step 0.3361325219273567\n",
      "Loss from 4220 to 4240 step 0.31769615411758423\n",
      "Loss from 4240 to 4260 step 0.31862071454524993\n",
      "Loss from 4260 to 4280 step 0.31697349697351457\n",
      "Loss from 4280 to 4300 step 0.32112318873405454\n",
      "Loss from 4300 to 4320 step 0.3450882151722908\n",
      "Loss from 4320 to 4340 step 0.31987963765859606\n",
      "Loss from 4340 to 4360 step 0.34571744352579115\n",
      "Loss from 4360 to 4380 step 0.3211357265710831\n",
      "Loss from 4380 to 4400 step 0.31851177364587785\n",
      "Loss from 4400 to 4420 step 0.3553940862417221\n",
      "Loss from 4420 to 4440 step 0.307234326004982\n",
      "Loss from 4440 to 4460 step 0.32031709998846053\n",
      "Loss from 4460 to 4480 step 0.3405205771327019\n",
      "Loss from 4480 to 4500 step 0.31174741089344027\n",
      "Loss from 4500 to 4520 step 0.333344592154026\n",
      "Loss from 4520 to 4540 step 0.32847309708595274\n",
      "Loss from 4540 to 4560 step 0.3119535431265831\n",
      "Loss from 4560 to 4580 step 0.33847261369228365\n",
      "Loss from 4580 to 4600 step 0.32190082520246505\n",
      "Loss from 4600 to 4620 step 0.3049734249711037\n",
      "Loss from 4620 to 4640 step 0.3126672014594078\n",
      "Loss from 4640 to 4660 step 0.3125536248087883\n",
      "Loss from 4660 to 4680 step 0.38411538749933244\n",
      "Loss from 4680 to 4700 step 0.3228713795542717\n",
      "Loss from 4700 to 4720 step 0.3731313794851303\n",
      "Loss from 4720 to 4740 step 0.33836047202348707\n",
      "Loss from 4740 to 4760 step 0.3063941389322281\n",
      "Loss from 4760 to 4780 step 0.3061323881149292\n",
      "Loss from 4780 to 4800 step 0.3212173983454704\n",
      "Loss from 4800 to 4820 step 0.3346286877989769\n",
      "Loss from 4820 to 4840 step 0.3247543707489967\n",
      "Loss from 4840 to 4860 step 0.38391647338867185\n",
      "Loss from 4860 to 4880 step 0.32702953219413755\n",
      "Loss from 4880 to 4900 step 0.31779061257839203\n",
      "Loss from 4900 to 4920 step 0.31809172630310056\n",
      "Loss from 4920 to 4940 step 0.3082030892372131\n",
      "Loss from 4940 to 4960 step 0.3336329236626625\n",
      "Loss from 4960 to 4980 step 0.32144694328308104\n",
      "Loss from 4980 to 5000 step 0.3467704996466637\n",
      "Loss from 5000 to 5020 step 0.31100131273269654\n",
      "Loss from 5020 to 5040 step 0.3153641074895859\n",
      "Loss from 5040 to 5060 step 0.31726464331150056\n",
      "Loss from 5060 to 5080 step 0.3619982048869133\n",
      "Loss from 5080 to 5100 step 0.316503968834877\n",
      "Loss from 5100 to 5120 step 0.34787587225437167\n",
      "Loss from 5120 to 5140 step 0.32486242800951004\n",
      "Loss from 5140 to 5160 step 0.4898386374115944\n",
      "Loss from 5160 to 5180 step 0.33275020122528076\n",
      "Loss from 5180 to 5200 step 0.30532977133989336\n",
      "Loss from 5200 to 5220 step 0.43394349068403243\n",
      "Loss from 5220 to 5240 step 0.4185190320014954\n",
      "Loss from 5240 to 5260 step 0.3048933669924736\n",
      "Loss from 5260 to 5280 step 0.3335864096879959\n",
      "Loss from 5280 to 5300 step 0.3425223663449287\n",
      "Loss from 5300 to 5320 step 0.2959897369146347\n",
      "Loss from 5320 to 5340 step 0.3004174143075943\n",
      "Loss from 5340 to 5360 step 0.30351887494325636\n",
      "Loss from 5360 to 5380 step 0.31698879450559614\n",
      "Loss from 5380 to 5400 step 0.29676284492015836\n",
      "Loss from 5400 to 5420 step 0.30834820717573164\n",
      "Loss from 5420 to 5440 step 0.30374129563570024\n",
      "Loss from 5440 to 5460 step 0.31505215764045713\n",
      "Loss from 5460 to 5480 step 0.2990233778953552\n",
      "Loss from 5480 to 5500 step 0.3324186235666275\n",
      "Loss from 5500 to 5520 step 0.41567832976579666\n",
      "Loss from 5520 to 5540 step 0.30888104885816575\n",
      "Loss from 5540 to 5560 step 0.3103617012500763\n",
      "Loss from 5560 to 5580 step 0.30292372554540636\n",
      "Loss from 5580 to 5600 step 0.3047103926539421\n",
      "Loss from 5600 to 5620 step 0.30831957757472994\n",
      "Loss from 5620 to 5640 step 0.37478039264678953\n",
      "Loss from 5640 to 5660 step 0.32944944202899934\n",
      "Loss from 5660 to 5680 step 0.3007089585065842\n",
      "Loss from 5680 to 5700 step 0.3265805780887604\n",
      "Loss from 5700 to 5720 step 0.305065843462944\n",
      "Loss from 5720 to 5740 step 0.30697770416736603\n",
      "Loss from 5740 to 5760 step 0.35011743009090424\n",
      "Loss from 5760 to 5780 step 0.3015755623579025\n",
      "Loss from 5780 to 5800 step 0.30824209898710253\n",
      "Loss from 5800 to 5820 step 0.30986100137233735\n",
      "Loss from 5820 to 5840 step 0.30891609787940977\n",
      "Loss from 5840 to 5860 step 0.3056462213397026\n",
      "Loss from 5860 to 5880 step 0.3135156467556953\n",
      "Loss from 5880 to 5900 step 0.32954633682966233\n",
      "Loss from 5900 to 5920 step 0.3361622229218483\n",
      "Loss from 5920 to 5940 step 0.3065702855587006\n",
      "Loss from 5940 to 5960 step 0.3130255609750748\n",
      "Loss from 5960 to 5980 step 0.30512113571166993\n",
      "Loss from 5980 to 6000 step 0.37111444026231766\n",
      "Loss from 6000 to 6020 step 0.30581194907426834\n",
      "Loss from 6020 to 6040 step 0.31617700308561325\n",
      "Loss from 6040 to 6060 step 0.29869904816150666\n",
      "Loss from 6060 to 6080 step 0.3413503900170326\n",
      "Loss from 6080 to 6100 step 0.31358629912137986\n",
      "Loss from 6100 to 6120 step 0.29704809188842773\n",
      "Loss from 6120 to 6140 step 0.2946242690086365\n",
      "Loss from 6140 to 6160 step 0.3030091911554337\n",
      "Loss from 6160 to 6180 step 0.3044202834367752\n",
      "Loss from 6180 to 6200 step 0.3019594818353653\n",
      "Loss from 6200 to 6220 step 0.30507518723607063\n",
      "Loss from 6220 to 6240 step 0.31392866373062134\n",
      "Loss from 6240 to 6260 step 0.3264938950538635\n",
      "Loss from 6260 to 6280 step 0.30369346141815184\n",
      "Loss from 6280 to 6300 step 0.3160232022404671\n",
      "Loss from 6300 to 6320 step 0.3031839609146118\n",
      "Loss from 6320 to 6340 step 0.3364811599254608\n",
      "Loss from 6340 to 6360 step 0.32280227690935137\n",
      "Loss from 6360 to 6380 step 0.29879275262355803\n",
      "Loss from 6380 to 6400 step 0.3236492395401001\n",
      "Loss from 6400 to 6420 step 0.4072383239865303\n",
      "Loss from 6420 to 6440 step 0.3077090188860893\n",
      "Loss from 6440 to 6460 step 0.32854124158620834\n",
      "Loss from 6460 to 6480 step 0.3420334532856941\n",
      "Loss from 6480 to 6500 step 0.303013676404953\n",
      "Loss from 6500 to 6520 step 0.317439778894186\n",
      "Loss from 6520 to 6540 step 0.3382440462708473\n",
      "Loss from 6540 to 6560 step 0.2993899241089821\n",
      "Loss from 6560 to 6580 step 0.30188877284526827\n",
      "Loss from 6580 to 6600 step 0.3179851233959198\n",
      "Loss from 6600 to 6620 step 0.315200112760067\n",
      "Loss from 6620 to 6640 step 0.31756394952535627\n",
      "Loss from 6640 to 6660 step 0.2910520523786545\n",
      "Loss from 6660 to 6680 step 0.34833078235387804\n",
      "Loss from 6680 to 6700 step 0.31986948996782305\n",
      "Loss from 6700 to 6720 step 0.3050043433904648\n",
      "Loss from 6720 to 6740 step 0.31157632917165756\n",
      "Loss from 6740 to 6760 step 0.29117671847343446\n",
      "Loss from 6760 to 6780 step 0.3282642737030983\n",
      "Loss from 6780 to 6800 step 0.299837751686573\n",
      "Loss from 6800 to 6820 step 0.33012387454509734\n",
      "Loss from 6820 to 6840 step 0.29640917629003527\n",
      "Loss from 6840 to 6860 step 0.30474098324775695\n",
      "Loss from 6860 to 6880 step 0.32206666469573975\n",
      "Loss from 6880 to 6900 step 0.3068449184298515\n",
      "Loss from 6900 to 6920 step 0.3149633392691612\n",
      "Loss from 6920 to 6940 step 0.30732847303152083\n",
      "Loss from 6940 to 6960 step 0.3859419211745262\n",
      "Loss from 6960 to 6980 step 0.39348156452178956\n",
      "Loss from 6980 to 7000 step 0.3605897337198257\n",
      "Loss from 7000 to 7020 step 0.33614500761032107\n",
      "Loss from 7020 to 7040 step 0.2839455395936966\n",
      "Loss from 7040 to 7060 step 0.31613824516534805\n",
      "Loss from 7060 to 7080 step 0.29871389716863633\n",
      "Loss from 7080 to 7100 step 0.3055315911769867\n",
      "Loss from 7100 to 7120 step 0.311898335814476\n",
      "Loss from 7120 to 7140 step 0.2978036105632782\n",
      "Loss from 7140 to 7160 step 0.3249414637684822\n",
      "Loss from 7160 to 7180 step 0.2966927945613861\n",
      "Loss from 7180 to 7200 step 0.2839038625359535\n",
      "Loss from 7200 to 7220 step 0.31123420894145964\n",
      "Loss from 7220 to 7240 step 0.2952630564570427\n",
      "Loss from 7240 to 7260 step 0.305826772749424\n",
      "Loss from 7260 to 7280 step 0.30214572995901107\n",
      "Loss from 7280 to 7300 step 0.3023614436388016\n",
      "Loss from 7300 to 7320 step 0.30112323760986326\n",
      "Loss from 7320 to 7340 step 0.2930677652359009\n",
      "Loss from 7340 to 7360 step 0.3002487227320671\n",
      "Loss from 7360 to 7380 step 0.3043984085321426\n",
      "Loss from 7380 to 7400 step 0.3033157750964165\n",
      "Loss from 7400 to 7420 step 0.3208598971366882\n",
      "Loss from 7420 to 7440 step 0.3119491472840309\n",
      "Loss from 7440 to 7460 step 0.31554475575685503\n",
      "Loss from 7460 to 7480 step 0.37488906681537626\n",
      "Loss from 7480 to 7500 step 0.3092349201440811\n",
      "Loss from 7500 to 7520 step 0.3068543657660484\n",
      "Loss from 7520 to 7540 step 0.37162542939186094\n",
      "Loss from 7540 to 7560 step 0.2910037711262703\n",
      "Loss from 7560 to 7580 step 0.2987891763448715\n",
      "Loss from 7580 to 7600 step 0.29303172379732134\n",
      "Loss from 7600 to 7620 step 0.29069817811250687\n",
      "Loss from 7620 to 7640 step 0.2962887346744537\n",
      "Loss from 7640 to 7660 step 0.31366617828607557\n",
      "Loss from 7660 to 7680 step 0.29822338968515394\n",
      "Loss from 7680 to 7700 step 0.32526383399963377\n",
      "Loss from 7700 to 7720 step 0.28366984724998473\n",
      "Loss from 7720 to 7740 step 0.2957712784409523\n",
      "Loss from 7740 to 7760 step 0.332261922955513\n",
      "Loss from 7760 to 7780 step 0.28334655314683915\n",
      "Loss from 7780 to 7800 step 0.292395131289959\n",
      "Loss from 7800 to 7820 step 0.292702978849411\n",
      "Loss from 7820 to 7840 step 0.28117767721414566\n",
      "Loss from 7840 to 7860 step 0.2993324875831604\n",
      "Loss from 7860 to 7880 step 0.30529090762138367\n",
      "Loss from 7880 to 7900 step 0.29269843697547915\n",
      "Loss from 7900 to 7920 step 0.2977189138531685\n",
      "Loss from 7920 to 7940 step 0.28203567266464236\n",
      "Loss from 7940 to 7960 step 0.3346617087721825\n",
      "Loss from 7960 to 7980 step 0.300141941010952\n",
      "Loss from 7980 to 8000 step 0.3007791444659233\n",
      "Loss from 8000 to 8020 step 0.3119624644517899\n",
      "Loss from 8020 to 8040 step 0.28616136461496355\n",
      "Loss from 8040 to 8060 step 0.3110338807106018\n",
      "Loss from 8060 to 8080 step 0.30070734173059466\n",
      "Loss from 8080 to 8100 step 0.31848307475447657\n",
      "Loss from 8100 to 8120 step 0.2840400844812393\n",
      "Loss from 8120 to 8140 step 0.2893516801297665\n",
      "Loss from 8140 to 8160 step 0.2999594762921333\n",
      "Loss from 8160 to 8180 step 0.28731955736875536\n",
      "Loss from 8180 to 8200 step 0.28364008069038393\n",
      "Loss from 8200 to 8220 step 0.295487803965807\n",
      "Loss from 8220 to 8240 step 0.2990508809685707\n",
      "Loss from 8240 to 8260 step 0.2987998753786087\n",
      "Loss from 8260 to 8280 step 0.28562274053692815\n",
      "Loss from 8280 to 8300 step 0.2785372294485569\n",
      "Loss from 8300 to 8320 step 0.29110011756420134\n",
      "Loss from 8320 to 8340 step 0.27989027872681616\n",
      "Loss from 8340 to 8360 step 0.29038387835025786\n",
      "Loss from 8360 to 8380 step 0.27132912129163744\n",
      "Loss from 8380 to 8400 step 0.3044493608176708\n",
      "Loss from 8400 to 8420 step 0.30019963905215263\n",
      "Loss from 8420 to 8440 step 0.3078226611018181\n",
      "Loss from 8440 to 8460 step 0.28761558383703234\n",
      "Loss from 8460 to 8480 step 0.2761419668793678\n",
      "Loss from 8480 to 8500 step 0.27033624425530434\n",
      "Loss from 8500 to 8520 step 0.27948228269815445\n",
      "Loss from 8520 to 8540 step 0.2740973874926567\n",
      "Loss from 8540 to 8560 step 0.2809984341263771\n",
      "Loss from 8560 to 8580 step 0.3184045433998108\n",
      "Loss from 8580 to 8600 step 0.28917538821697236\n",
      "Loss from 8600 to 8620 step 0.3212430410087109\n",
      "Loss from 8620 to 8640 step 0.28642806857824327\n",
      "Loss from 8640 to 8660 step 0.2815913312137127\n",
      "Loss from 8660 to 8680 step 0.32971729040145875\n",
      "Loss from 8680 to 8700 step 0.2775248780846596\n",
      "Loss from 8700 to 8720 step 0.2889917649328709\n",
      "Loss from 8720 to 8740 step 0.2924556121230125\n",
      "Loss from 8740 to 8760 step 0.27423680052161215\n",
      "Loss from 8760 to 8780 step 0.29985484182834626\n",
      "Loss from 8780 to 8800 step 0.2745684623718262\n",
      "Loss from 8800 to 8820 step 0.29801390022039415\n",
      "Loss from 8820 to 8840 step 0.29676914513111113\n",
      "Loss from 8840 to 8860 step 0.2803056091070175\n",
      "Loss from 8860 to 8880 step 0.2697733767330647\n",
      "Loss from 8880 to 8900 step 0.3046089880168438\n",
      "Loss from 8900 to 8920 step 0.26982908695936203\n",
      "Loss from 8920 to 8940 step 0.2707591071724892\n",
      "Loss from 8940 to 8960 step 0.26274944096803665\n",
      "Loss from 8960 to 8980 step 0.49115007370710373\n",
      "Loss from 8980 to 9000 step 0.3003352738916874\n",
      "Loss from 9000 to 9020 step 0.2792250715196133\n",
      "Loss from 9020 to 9040 step 0.31930446848273275\n",
      "Loss from 9040 to 9060 step 0.3033728957176208\n",
      "Loss from 9060 to 9080 step 0.27465375140309334\n",
      "Loss from 9080 to 9100 step 0.2735538318753242\n",
      "Loss from 9100 to 9120 step 0.30799621939659116\n",
      "Loss from 9120 to 9140 step 0.26497708559036254\n",
      "Loss from 9140 to 9160 step 0.2662476055324078\n",
      "Loss from 9160 to 9180 step 0.26473933458328247\n",
      "Loss from 9180 to 9200 step 0.2931947335600853\n",
      "Loss from 9200 to 9220 step 0.305195964127779\n",
      "Loss from 9220 to 9240 step 0.2841331705451012\n",
      "Loss from 9240 to 9260 step 0.2891549490392208\n",
      "Loss from 9260 to 9280 step 0.3413399748504162\n",
      "Loss from 9280 to 9300 step 0.265776077657938\n",
      "Loss from 9300 to 9320 step 0.270766519010067\n",
      "Loss from 9320 to 9340 step 0.2625940337777138\n",
      "Loss from 9340 to 9360 step 0.27102345824241636\n",
      "Loss from 9360 to 9380 step 0.26364466473460196\n",
      "Loss from 9380 to 9400 step 0.26139332801103593\n",
      "Loss from 9400 to 9420 step 0.2797292910516262\n",
      "Loss from 9420 to 9440 step 0.4283532343804836\n",
      "Loss from 9440 to 9460 step 0.31658210009336474\n",
      "Loss from 9460 to 9480 step 0.267631559073925\n",
      "Loss from 9480 to 9500 step 0.5737663447856903\n",
      "Loss from 9500 to 9520 step 0.28111706748604776\n",
      "Loss from 9520 to 9540 step 0.27951200529932974\n",
      "Loss from 9540 to 9560 step 0.26010169833898544\n",
      "Loss from 9560 to 9580 step 0.2614430904388428\n",
      "Loss from 9580 to 9600 step 0.28005815520882604\n",
      "Loss from 9600 to 9620 step 0.2749171920120716\n",
      "Loss from 9620 to 9640 step 0.2763736769556999\n",
      "Loss from 9640 to 9660 step 0.26088460460305213\n",
      "Loss from 9660 to 9680 step 0.2721413716673851\n",
      "Loss from 9680 to 9700 step 0.2672140546143055\n",
      "Loss from 9700 to 9720 step 0.3212316133081913\n",
      "Loss from 9720 to 9740 step 0.27488089352846146\n",
      "Loss from 9740 to 9760 step 0.2791342258453369\n",
      "Loss from 9760 to 9780 step 0.26462539210915564\n",
      "Loss from 9780 to 9800 step 0.2946644231677055\n",
      "Loss from 9800 to 9820 step 0.26895699352025987\n",
      "Loss from 9820 to 9840 step 0.2528104864060879\n",
      "Loss from 9840 to 9860 step 0.2833060652017593\n",
      "Loss from 9860 to 9880 step 0.25954975709319117\n",
      "Loss from 9880 to 9900 step 0.26395713686943056\n",
      "Loss from 9900 to 9920 step 0.26154941618442534\n",
      "Loss from 9920 to 9940 step 0.2750580057501793\n",
      "Loss from 9940 to 9960 step 0.28601310178637507\n",
      "Loss from 9960 to 9980 step 0.28138451278209686\n",
      "Loss from 9980 to 10000 step 0.3126159779727459\n",
      "Loss from 10000 to 10020 step 0.2670640453696251\n",
      "Loss from 10020 to 10040 step 0.2679572828114033\n",
      "Loss from 10040 to 10060 step 0.3696698881685734\n",
      "Loss from 10060 to 10080 step 0.26962467432022097\n",
      "Loss from 10080 to 10100 step 0.2901190832257271\n",
      "Loss from 10100 to 10120 step 0.2638654813170433\n",
      "Loss from 10120 to 10140 step 0.2691838353872299\n",
      "Loss from 10140 to 10160 step 0.25706985518336295\n",
      "Loss from 10160 to 10180 step 0.26130724251270293\n",
      "Loss from 10180 to 10200 step 0.33129442259669306\n",
      "Loss from 10200 to 10220 step 0.2619191199541092\n",
      "Loss from 10220 to 10240 step 0.2805024884641171\n",
      "Loss from 10240 to 10260 step 0.2606371209025383\n",
      "Loss from 10260 to 10280 step 0.26447405368089677\n",
      "Loss from 10280 to 10300 step 0.28702965676784514\n",
      "Loss from 10300 to 10320 step 0.2686992555856705\n",
      "Loss from 10320 to 10340 step 0.2650081664323807\n",
      "Loss from 10340 to 10360 step 0.2619163252413273\n",
      "Loss from 10360 to 10380 step 0.32474285885691645\n",
      "Loss from 10380 to 10400 step 0.2702496461570263\n",
      "Loss from 10400 to 10420 step 0.2631564952433109\n",
      "Loss from 10420 to 10440 step 0.2652291677892208\n",
      "Loss from 10440 to 10460 step 0.26763966828584673\n",
      "Loss from 10460 to 10480 step 0.2686695232987404\n",
      "Loss from 10480 to 10500 step 0.2597378149628639\n",
      "Loss from 10500 to 10520 step 0.28215867206454276\n",
      "Loss from 10520 to 10540 step 0.2613087922334671\n",
      "Loss from 10540 to 10560 step 0.2586455062031746\n",
      "Loss from 10560 to 10580 step 0.24932425394654273\n",
      "Loss from 10580 to 10600 step 0.26202034428715704\n",
      "Loss from 10600 to 10620 step 0.24977308362722397\n",
      "Loss from 10620 to 10640 step 0.29136972725391386\n",
      "Loss from 10640 to 10660 step 0.25520493984222414\n",
      "Loss from 10660 to 10680 step 0.36600212901830675\n",
      "Loss from 10680 to 10700 step 0.3741267703473568\n",
      "Loss from 10700 to 10720 step 0.26767577081918714\n",
      "Loss from 10720 to 10740 step 0.24950050339102745\n",
      "Loss from 10740 to 10760 step 0.31870223954319954\n",
      "Loss from 10760 to 10780 step 0.25384178310632705\n",
      "Loss from 10780 to 10800 step 0.23772067576646805\n",
      "Loss from 10800 to 10820 step 0.24817187190055848\n",
      "Loss from 10820 to 10840 step 0.2597906470298767\n",
      "Loss from 10840 to 10860 step 0.2561580643057823\n",
      "Loss from 10860 to 10880 step 0.30019266083836554\n",
      "Loss from 10880 to 10900 step 0.2801437020301819\n",
      "Loss from 10900 to 10920 step 0.24099857732653618\n",
      "Loss from 10920 to 10940 step 0.2790695622563362\n",
      "Loss from 10940 to 10960 step 0.27375454977154734\n",
      "Loss from 10960 to 10980 step 0.5976691164076329\n",
      "Loss from 10980 to 11000 step 0.26066107898950575\n",
      "Loss from 11000 to 11020 step 0.2456291474401951\n",
      "Loss from 11020 to 11040 step 0.24902356192469596\n",
      "Loss from 11040 to 11060 step 0.23690025806427\n",
      "Loss from 11060 to 11080 step 0.3191042743623257\n",
      "Loss from 11080 to 11100 step 0.2932502880692482\n",
      "Loss from 11100 to 11120 step 0.2524580262601376\n",
      "Loss from 11120 to 11140 step 0.24994439855217934\n",
      "Loss from 11140 to 11160 step 0.25025718510150907\n",
      "Loss from 11160 to 11180 step 0.24360274374485016\n",
      "Loss from 11180 to 11200 step 0.25493298247456553\n",
      "Loss from 11200 to 11220 step 0.2853955030441284\n",
      "Loss from 11220 to 11240 step 0.25143336653709414\n",
      "Loss from 11240 to 11260 step 0.23226174861192703\n",
      "Loss from 11260 to 11280 step 0.24298005923628807\n",
      "Loss from 11280 to 11300 step 0.25722524598240853\n",
      "Loss from 11300 to 11320 step 0.23771961107850076\n",
      "Loss from 11320 to 11340 step 0.23037046194076538\n",
      "Loss from 11340 to 11360 step 0.24327481389045716\n",
      "Loss from 11360 to 11380 step 0.2818983934819698\n",
      "Loss from 11380 to 11400 step 0.2765040621161461\n",
      "Loss from 11400 to 11420 step 0.24200730621814728\n",
      "Loss from 11420 to 11440 step 0.2353955738246441\n",
      "Loss from 11440 to 11460 step 0.24644609689712524\n",
      "Loss from 11460 to 11480 step 0.24100245907902718\n",
      "Loss from 11480 to 11500 step 0.23664459139108657\n",
      "Loss from 11500 to 11520 step 0.2569583110511303\n",
      "Loss from 11520 to 11540 step 0.23371901065111161\n",
      "Loss from 11540 to 11560 step 0.23157447800040246\n",
      "Loss from 11560 to 11580 step 0.28149977400898935\n",
      "Loss from 11580 to 11600 step 0.24541454017162323\n",
      "Loss from 11600 to 11620 step 0.2547975927591324\n",
      "Loss from 11620 to 11640 step 0.2555611178278923\n",
      "Loss from 11640 to 11660 step 0.2558545544743538\n",
      "Loss from 11660 to 11680 step 0.22690697088837625\n",
      "Loss from 11680 to 11700 step 0.23075167909264566\n",
      "Loss from 11700 to 11720 step 0.26476816907525064\n",
      "Loss from 11720 to 11740 step 0.29040714651346206\n",
      "Loss from 11740 to 11760 step 0.2599579818546772\n",
      "Loss from 11760 to 11780 step 0.25237882882356644\n",
      "Loss from 11780 to 11800 step 0.24730875492095947\n",
      "Loss from 11800 to 11820 step 0.24498139917850495\n",
      "Loss from 11820 to 11840 step 0.2600935369729996\n",
      "Loss from 11840 to 11860 step 0.2532586805522442\n",
      "Loss from 11860 to 11880 step 0.2393575631082058\n",
      "Loss from 11880 to 11900 step 0.23780705854296685\n",
      "Loss from 11900 to 11920 step 0.23534609675407409\n",
      "Loss from 11920 to 11940 step 0.24425737485289573\n",
      "Loss from 11940 to 11960 step 0.23423618152737619\n",
      "Loss from 11960 to 11980 step 0.24930565506219865\n",
      "Loss from 11980 to 12000 step 0.23543111458420754\n",
      "Loss from 12000 to 12020 step 0.24150005578994752\n",
      "Loss from 12020 to 12040 step 0.2627273865044117\n",
      "Loss from 12040 to 12060 step 0.26500791162252424\n",
      "Loss from 12060 to 12080 step 0.2312004216015339\n",
      "Loss from 12080 to 12100 step 0.24628870859742164\n",
      "Loss from 12100 to 12120 step 0.24745795875787735\n",
      "Loss from 12120 to 12140 step 0.225742419809103\n",
      "Loss from 12140 to 12160 step 0.2609068140387535\n",
      "Loss from 12160 to 12180 step 0.22143580093979837\n",
      "Loss from 12180 to 12200 step 0.24230189397931098\n",
      "Loss from 12200 to 12220 step 0.23696434050798415\n",
      "Loss from 12220 to 12240 step 0.22963785976171494\n",
      "Loss from 12240 to 12260 step 0.23674097284674644\n",
      "Loss from 12260 to 12280 step 0.21776276752352713\n",
      "Loss from 12280 to 12300 step 0.2490486890077591\n",
      "Loss from 12300 to 12320 step 0.2828567199409008\n",
      "Loss from 12320 to 12340 step 0.22996568381786348\n",
      "Loss from 12340 to 12360 step 0.23487715497612954\n",
      "Loss from 12360 to 12380 step 0.22040412500500678\n",
      "Loss from 12380 to 12400 step 0.2332687944173813\n",
      "Loss from 12400 to 12420 step 0.21108847111463547\n",
      "Loss from 12420 to 12440 step 0.2895187519490719\n",
      "Loss from 12440 to 12460 step 0.24467062056064606\n",
      "Loss from 12460 to 12480 step 0.22496796995401383\n",
      "Loss from 12480 to 12500 step 0.22310080826282502\n",
      "Train loss: 0.3101287987291813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [01:59<00:00, 834.42 examples/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f0a6e2ceb64039a7884b66de6caab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss from 0 to 20 step 0.21620865762233735\n",
      "Loss from 20 to 40 step 0.23154037520289422\n",
      "Loss from 40 to 60 step 0.23773094341158868\n",
      "Loss from 60 to 80 step 0.22272755578160286\n",
      "Loss from 80 to 100 step 0.2311197392642498\n",
      "Loss from 100 to 120 step 0.22758381590247154\n",
      "Loss from 120 to 140 step 0.2201256826519966\n",
      "Loss from 140 to 160 step 0.24708592891693115\n",
      "Loss from 160 to 180 step 0.22181706205010415\n",
      "Loss from 180 to 200 step 0.21560477912425996\n",
      "Loss from 200 to 220 step 0.22340274453163148\n",
      "Loss from 220 to 240 step 0.23231444433331488\n",
      "Loss from 240 to 260 step 0.23521733433008193\n",
      "Loss from 260 to 280 step 0.24628016129136085\n",
      "Loss from 280 to 300 step 0.22454896569252014\n",
      "Loss from 300 to 320 step 0.2787293650209904\n",
      "Loss from 320 to 340 step 0.20748767703771592\n",
      "Loss from 340 to 360 step 0.20392531231045724\n",
      "Loss from 360 to 380 step 0.25865465477108956\n",
      "Loss from 380 to 400 step 0.22991502285003662\n",
      "Loss from 400 to 420 step 0.24059441909193993\n",
      "Loss from 420 to 440 step 0.20755440816283227\n",
      "Loss from 440 to 460 step 0.23467788845300674\n",
      "Loss from 460 to 480 step 0.20254052206873893\n",
      "Loss from 480 to 500 step 0.23760280683636664\n",
      "Loss from 500 to 520 step 0.2024159163236618\n",
      "Loss from 520 to 540 step 0.21458470225334167\n",
      "Loss from 540 to 560 step 0.21433867812156676\n",
      "Loss from 560 to 580 step 0.22203753851354122\n",
      "Loss from 580 to 600 step 0.21469878926873207\n",
      "Loss from 600 to 620 step 0.22116690799593924\n",
      "Loss from 620 to 640 step 0.2296653524041176\n",
      "Loss from 640 to 660 step 0.21892226487398148\n",
      "Loss from 660 to 680 step 0.23157491013407708\n",
      "Loss from 680 to 700 step 0.2075339674949646\n",
      "Loss from 700 to 720 step 0.21924628391861917\n",
      "Loss from 720 to 740 step 0.19750159084796906\n",
      "Loss from 740 to 760 step 0.24851650446653367\n",
      "Loss from 760 to 780 step 0.21082798019051552\n",
      "Loss from 780 to 800 step 0.23921952173113822\n",
      "Loss from 800 to 820 step 0.29999070093035696\n",
      "Loss from 820 to 840 step 0.24233022555708886\n",
      "Loss from 840 to 860 step 0.19713269099593161\n",
      "Loss from 860 to 880 step 0.22197663709521293\n",
      "Loss from 880 to 900 step 0.20975124090909958\n",
      "Loss from 900 to 920 step 0.20532494634389878\n",
      "Loss from 920 to 940 step 0.2129846379160881\n",
      "Loss from 940 to 960 step 0.21607675999403\n",
      "Loss from 960 to 980 step 0.22898229584097862\n",
      "Loss from 980 to 1000 step 0.22063075751066208\n",
      "Loss from 1000 to 1020 step 0.24117975309491158\n",
      "Loss from 1020 to 1040 step 0.21039261370897294\n",
      "Loss from 1040 to 1060 step 0.22061822935938835\n",
      "Loss from 1060 to 1080 step 0.20941432788968087\n",
      "Loss from 1080 to 1100 step 0.19694969952106475\n",
      "Loss from 1100 to 1120 step 0.2346420779824257\n",
      "Loss from 1120 to 1140 step 0.20775910541415216\n",
      "Loss from 1140 to 1160 step 0.20334375500679017\n",
      "Loss from 1160 to 1180 step 0.19908626675605773\n",
      "Loss from 1180 to 1200 step 0.22735275626182555\n",
      "Loss from 1200 to 1220 step 0.2090321309864521\n",
      "Loss from 1220 to 1240 step 0.22486503496766092\n",
      "Loss from 1240 to 1260 step 0.21563988626003266\n",
      "Loss from 1260 to 1280 step 0.20453478544950485\n",
      "Loss from 1280 to 1300 step 0.2219017431139946\n",
      "Loss from 1300 to 1320 step 0.20337095186114312\n",
      "Loss from 1320 to 1340 step 0.19200179800391198\n",
      "Loss from 1340 to 1360 step 0.200100889056921\n",
      "Loss from 1360 to 1380 step 0.1978554166853428\n",
      "Loss from 1380 to 1400 step 0.1884546235203743\n",
      "Loss from 1400 to 1420 step 0.21400344073772432\n",
      "Loss from 1420 to 1440 step 0.2165355511009693\n",
      "Loss from 1440 to 1460 step 0.3253102652728558\n",
      "Loss from 1460 to 1480 step 0.21370963454246522\n",
      "Loss from 1480 to 1500 step 0.1847640812397003\n",
      "Loss from 1500 to 1520 step 0.19590277820825577\n",
      "Loss from 1520 to 1540 step 0.22466885298490524\n",
      "Loss from 1540 to 1560 step 0.21177014335989952\n",
      "Loss from 1560 to 1580 step 0.20501230731606485\n",
      "Loss from 1580 to 1600 step 0.20641110837459564\n",
      "Loss from 1600 to 1620 step 0.18759374022483827\n",
      "Loss from 1620 to 1640 step 0.21955742985010146\n",
      "Loss from 1640 to 1660 step 0.19777210280299187\n",
      "Loss from 1660 to 1680 step 0.19942954629659654\n",
      "Loss from 1680 to 1700 step 0.20681732296943664\n",
      "Loss from 1700 to 1720 step 0.183505018055439\n",
      "Loss from 1720 to 1740 step 0.19072271808981894\n",
      "Loss from 1740 to 1760 step 0.19226860627532005\n",
      "Loss from 1760 to 1780 step 0.20137355402112006\n",
      "Loss from 1780 to 1800 step 0.20813225358724594\n",
      "Loss from 1800 to 1820 step 0.1940018817782402\n",
      "Loss from 1820 to 1840 step 0.20526931658387185\n",
      "Loss from 1840 to 1860 step 0.20728119015693663\n",
      "Loss from 1860 to 1880 step 0.21336836665868758\n",
      "Loss from 1880 to 1900 step 0.1928030475974083\n",
      "Loss from 1900 to 1920 step 0.19887032955884934\n",
      "Loss from 1920 to 1940 step 0.20321854650974275\n",
      "Loss from 1940 to 1960 step 0.19092934355139732\n",
      "Loss from 1960 to 1980 step 0.19129674434661864\n",
      "Loss from 1980 to 2000 step 0.23709720447659494\n",
      "Loss from 2000 to 2020 step 0.23798392787575723\n",
      "Loss from 2020 to 2040 step 0.19532934650778772\n",
      "Loss from 2040 to 2060 step 0.2272389106452465\n",
      "Loss from 2060 to 2080 step 0.19954711496829985\n",
      "Loss from 2080 to 2100 step 0.18625950291752816\n",
      "Loss from 2100 to 2120 step 0.20308108031749725\n",
      "Loss from 2120 to 2140 step 0.22211840376257896\n",
      "Loss from 2140 to 2160 step 0.21195239052176476\n",
      "Loss from 2160 to 2180 step 0.21676622480154037\n",
      "Loss from 2180 to 2200 step 0.1797075890004635\n",
      "Loss from 2200 to 2220 step 0.20371486395597457\n",
      "Loss from 2220 to 2240 step 0.18910735175013543\n",
      "Loss from 2240 to 2260 step 0.19368333742022514\n",
      "Loss from 2260 to 2280 step 0.1949911966919899\n",
      "Loss from 2280 to 2300 step 0.2238439902663231\n",
      "Loss from 2300 to 2320 step 0.18924088329076766\n",
      "Loss from 2320 to 2340 step 0.19107475280761718\n",
      "Loss from 2340 to 2360 step 0.225119436532259\n",
      "Loss from 2360 to 2380 step 0.1802773132920265\n",
      "Loss from 2380 to 2400 step 0.1813138820230961\n",
      "Loss from 2400 to 2420 step 0.17976479679346086\n",
      "Loss from 2420 to 2440 step 0.19172649085521698\n",
      "Loss from 2440 to 2460 step 0.18549900650978088\n",
      "Loss from 2460 to 2480 step 0.18954183533787727\n",
      "Loss from 2480 to 2500 step 0.19227642565965652\n",
      "Loss from 2500 to 2520 step 0.19182187914848328\n",
      "Loss from 2520 to 2540 step 0.188477623462677\n",
      "Loss from 2540 to 2560 step 0.19913376346230507\n",
      "Loss from 2560 to 2580 step 0.18558656871318818\n",
      "Loss from 2580 to 2600 step 0.1944805420935154\n",
      "Loss from 2600 to 2620 step 0.20725458413362502\n",
      "Loss from 2620 to 2640 step 0.19509269520640374\n",
      "Loss from 2640 to 2660 step 0.19335583075881005\n",
      "Loss from 2660 to 2680 step 0.18035977110266685\n",
      "Loss from 2680 to 2700 step 0.1863752067089081\n",
      "Loss from 2700 to 2720 step 0.18875882551074027\n",
      "Loss from 2720 to 2740 step 0.1867057867348194\n",
      "Loss from 2740 to 2760 step 0.1941939577460289\n",
      "Loss from 2760 to 2780 step 0.19424351453781127\n",
      "Loss from 2780 to 2800 step 0.19043708592653275\n",
      "Loss from 2800 to 2820 step 0.18906293213367462\n",
      "Loss from 2820 to 2840 step 0.2224743328988552\n",
      "Loss from 2840 to 2860 step 0.2106767050921917\n",
      "Loss from 2860 to 2880 step 0.19764157757163048\n",
      "Loss from 2880 to 2900 step 0.31879055947065355\n",
      "Loss from 2900 to 2920 step 0.19527423307299613\n",
      "Loss from 2920 to 2940 step 0.2542115435004234\n",
      "Loss from 2940 to 2960 step 0.1992504581809044\n",
      "Loss from 2960 to 2980 step 0.1722143404185772\n",
      "Loss from 2980 to 3000 step 0.16947478875517846\n",
      "Loss from 3000 to 3020 step 0.19645825475454332\n",
      "Loss from 3020 to 3040 step 0.24730510860681534\n",
      "Loss from 3040 to 3060 step 0.20211075097322465\n",
      "Loss from 3060 to 3080 step 0.19080961868166924\n",
      "Loss from 3080 to 3100 step 0.18883529007434846\n",
      "Loss from 3100 to 3120 step 0.18150197640061377\n",
      "Loss from 3120 to 3140 step 0.30731719061732293\n",
      "Loss from 3140 to 3160 step 0.1838404953479767\n",
      "Loss from 3160 to 3180 step 0.18337761759757995\n",
      "Loss from 3180 to 3200 step 0.22446335554122926\n",
      "Loss from 3200 to 3220 step 0.18113735616207122\n",
      "Loss from 3220 to 3240 step 0.17859798520803452\n",
      "Loss from 3240 to 3260 step 0.18134307861328125\n",
      "Loss from 3260 to 3280 step 0.1712590955197811\n",
      "Loss from 3280 to 3300 step 0.17475765943527222\n",
      "Loss from 3300 to 3320 step 0.16760984063148499\n",
      "Loss from 3320 to 3340 step 0.19736630693078042\n",
      "Loss from 3340 to 3360 step 0.16905689164996146\n",
      "Loss from 3360 to 3380 step 0.16988916769623758\n",
      "Loss from 3380 to 3400 step 0.1778322920203209\n",
      "Loss from 3400 to 3420 step 0.17146951779723169\n",
      "Loss from 3420 to 3440 step 0.17549945786595345\n",
      "Loss from 3440 to 3460 step 0.2063874199986458\n",
      "Loss from 3460 to 3480 step 0.1836311250925064\n",
      "Loss from 3480 to 3500 step 0.17725388407707215\n",
      "Loss from 3500 to 3520 step 0.18473186902701855\n",
      "Loss from 3520 to 3540 step 0.19122365787625312\n",
      "Loss from 3540 to 3560 step 0.17522229477763177\n",
      "Loss from 3560 to 3580 step 0.1789709821343422\n",
      "Loss from 3580 to 3600 step 0.17220247238874437\n",
      "Loss from 3600 to 3620 step 0.22292422950267793\n",
      "Loss from 3620 to 3640 step 0.16958658620715142\n",
      "Loss from 3640 to 3660 step 0.17287049070000648\n",
      "Loss from 3660 to 3680 step 0.18335213959217073\n",
      "Loss from 3680 to 3700 step 0.1830172099173069\n",
      "Loss from 3700 to 3720 step 0.18764557763934137\n",
      "Loss from 3720 to 3740 step 0.16447957456111909\n",
      "Loss from 3740 to 3760 step 0.1704878941178322\n",
      "Loss from 3760 to 3780 step 0.22012564912438393\n",
      "Loss from 3780 to 3800 step 0.19554457440972328\n",
      "Loss from 3800 to 3820 step 0.21520819440484046\n",
      "Loss from 3820 to 3840 step 0.16507291197776794\n",
      "Loss from 3840 to 3860 step 0.16810209825634956\n",
      "Loss from 3860 to 3880 step 0.16098895221948623\n",
      "Loss from 3880 to 3900 step 0.1634014867246151\n",
      "Loss from 3900 to 3920 step 0.16708005554974079\n",
      "Loss from 3920 to 3940 step 0.282706742733717\n",
      "Loss from 3940 to 3960 step 0.1580788806080818\n",
      "Loss from 3960 to 3980 step 0.15340569466352463\n",
      "Loss from 3980 to 4000 step 0.179092887789011\n",
      "Loss from 4000 to 4020 step 0.20702437087893485\n",
      "Loss from 4020 to 4040 step 0.18088574558496476\n",
      "Loss from 4040 to 4060 step 0.20856360234320165\n",
      "Loss from 4060 to 4080 step 0.17092875391244888\n",
      "Loss from 4080 to 4100 step 0.15927493199706078\n",
      "Loss from 4100 to 4120 step 0.16969305016100406\n",
      "Loss from 4120 to 4140 step 0.16015172749757767\n",
      "Loss from 4140 to 4160 step 0.17558510974049568\n",
      "Loss from 4160 to 4180 step 0.16978594698011876\n",
      "Loss from 4180 to 4200 step 0.17703604325652122\n",
      "Loss from 4200 to 4220 step 0.1562568262219429\n",
      "Loss from 4220 to 4240 step 0.14664479531347752\n",
      "Loss from 4240 to 4260 step 0.21879564225673676\n",
      "Loss from 4260 to 4280 step 0.15679695457220078\n",
      "Loss from 4280 to 4300 step 0.150519210845232\n",
      "Loss from 4300 to 4320 step 0.14860572069883346\n",
      "Loss from 4320 to 4340 step 0.15257550179958343\n",
      "Loss from 4340 to 4360 step 0.16385163739323616\n",
      "Loss from 4360 to 4380 step 0.14582191705703734\n",
      "Loss from 4380 to 4400 step 0.1498164039105177\n",
      "Loss from 4400 to 4420 step 0.1460360761731863\n",
      "Loss from 4420 to 4440 step 0.16032872013747693\n",
      "Loss from 4440 to 4460 step 0.14822670742869376\n",
      "Loss from 4460 to 4480 step 0.1681361969560385\n",
      "Loss from 4480 to 4500 step 0.14413281194865704\n",
      "Loss from 4500 to 4520 step 0.14793958850204944\n",
      "Loss from 4520 to 4540 step 0.1581046961247921\n",
      "Loss from 4540 to 4560 step 0.14767262749373913\n",
      "Loss from 4560 to 4580 step 0.1461051046848297\n",
      "Loss from 4580 to 4600 step 0.16196335405111312\n",
      "Loss from 4600 to 4620 step 0.15145661681890488\n",
      "Loss from 4620 to 4640 step 0.1614884652197361\n",
      "Loss from 4640 to 4660 step 0.1487533211708069\n",
      "Loss from 4660 to 4680 step 0.14011649340391158\n",
      "Loss from 4680 to 4700 step 0.19020258337259294\n",
      "Loss from 4700 to 4720 step 0.14556697458028794\n",
      "Loss from 4720 to 4740 step 0.162161286175251\n",
      "Loss from 4740 to 4760 step 0.15067526027560235\n",
      "Loss from 4760 to 4780 step 0.15336998626589776\n",
      "Loss from 4780 to 4800 step 0.1634003732353449\n",
      "Loss from 4800 to 4820 step 0.1664996389299631\n",
      "Loss from 4820 to 4840 step 0.14864471852779387\n",
      "Loss from 4840 to 4860 step 0.36455344073474405\n",
      "Loss from 4860 to 4880 step 0.14387553930282593\n",
      "Loss from 4880 to 4900 step 0.1691335268318653\n",
      "Loss from 4900 to 4920 step 0.13950669765472412\n",
      "Loss from 4920 to 4940 step 0.16027665734291077\n",
      "Loss from 4940 to 4960 step 0.19296478517353535\n",
      "Loss from 4960 to 4980 step 0.15623424239456654\n",
      "Loss from 4980 to 5000 step 0.15259208232164384\n",
      "Loss from 5000 to 5020 step 0.13345585614442826\n",
      "Loss from 5020 to 5040 step 0.14022209756076337\n",
      "Loss from 5040 to 5060 step 0.15049652606248856\n",
      "Loss from 5060 to 5080 step 0.20147464647889138\n",
      "Loss from 5080 to 5100 step 0.16678617112338542\n",
      "Loss from 5100 to 5120 step 0.14327059015631677\n",
      "Loss from 5120 to 5140 step 0.14595221504569053\n",
      "Loss from 5140 to 5160 step 0.13615225590765476\n",
      "Loss from 5160 to 5180 step 0.13916402086615562\n",
      "Loss from 5180 to 5200 step 0.16360662765800954\n",
      "Loss from 5200 to 5220 step 0.18163258098065854\n",
      "Loss from 5220 to 5240 step 0.14523841477930546\n",
      "Loss from 5240 to 5260 step 0.13679871410131456\n",
      "Loss from 5260 to 5280 step 0.1324060432612896\n",
      "Loss from 5280 to 5300 step 0.16469160057604312\n",
      "Loss from 5300 to 5320 step 0.14112621657550334\n",
      "Loss from 5320 to 5340 step 0.15497363582253457\n",
      "Loss from 5340 to 5360 step 0.15871499925851823\n",
      "Loss from 5360 to 5380 step 0.14711267314851284\n",
      "Loss from 5380 to 5400 step 0.14170329682528973\n",
      "Loss from 5400 to 5420 step 0.13803963586688042\n",
      "Loss from 5420 to 5440 step 0.14013375379145146\n",
      "Loss from 5440 to 5460 step 0.15821740292012693\n",
      "Loss from 5460 to 5480 step 0.13501399718225002\n",
      "Loss from 5480 to 5500 step 0.16165997721254827\n",
      "Loss from 5500 to 5520 step 0.18123944960534571\n",
      "Loss from 5520 to 5540 step 0.15335403606295586\n",
      "Loss from 5540 to 5560 step 0.14879990965127946\n",
      "Loss from 5560 to 5580 step 0.1557178322225809\n",
      "Loss from 5580 to 5600 step 0.1345996480435133\n",
      "Loss from 5600 to 5620 step 0.14173492714762687\n",
      "Loss from 5620 to 5640 step 0.1600929919630289\n",
      "Loss from 5640 to 5660 step 0.16917580142617225\n",
      "Loss from 5660 to 5680 step 0.1637647945433855\n",
      "Loss from 5680 to 5700 step 0.1324801232665777\n",
      "Loss from 5700 to 5720 step 0.13038224391639233\n",
      "Loss from 5720 to 5740 step 0.17663328796625138\n",
      "Loss from 5740 to 5760 step 0.14234701357781887\n",
      "Loss from 5760 to 5780 step 0.1284322053194046\n",
      "Loss from 5780 to 5800 step 0.14007655084133147\n",
      "Loss from 5800 to 5820 step 0.13512449637055396\n",
      "Loss from 5820 to 5840 step 0.13425967022776603\n",
      "Loss from 5840 to 5860 step 0.14260375872254372\n",
      "Loss from 5860 to 5880 step 0.13460193052887917\n",
      "Loss from 5880 to 5900 step 0.1532214693725109\n",
      "Loss from 5900 to 5920 step 0.1454066324979067\n",
      "Loss from 5920 to 5940 step 0.13833403438329697\n",
      "Loss from 5940 to 5960 step 0.15518236123025417\n",
      "Loss from 5960 to 5980 step 0.13189960457384586\n",
      "Loss from 5980 to 6000 step 0.12674232870340346\n",
      "Loss from 6000 to 6020 step 0.1264092270284891\n",
      "Loss from 6020 to 6040 step 0.13982613682746886\n",
      "Loss from 6040 to 6060 step 0.16701030991971494\n",
      "Loss from 6060 to 6080 step 0.14055332355201244\n",
      "Loss from 6080 to 6100 step 0.1466987170279026\n",
      "Loss from 6100 to 6120 step 0.14200556166470052\n",
      "Loss from 6120 to 6140 step 0.1404940877109766\n",
      "Loss from 6140 to 6160 step 0.15741889476776122\n",
      "Loss from 6160 to 6180 step 0.13058328181505202\n",
      "Loss from 6180 to 6200 step 0.1352490559220314\n",
      "Loss from 6200 to 6220 step 0.1537752877920866\n",
      "Loss from 6220 to 6240 step 0.16021382287144662\n",
      "Loss from 6240 to 6260 step 0.12633186243474484\n",
      "Loss from 6260 to 6280 step 0.18173621483147145\n",
      "Loss from 6280 to 6300 step 0.21398875415325164\n",
      "Loss from 6300 to 6320 step 0.1684128325432539\n",
      "Loss from 6320 to 6340 step 0.130420034378767\n",
      "Loss from 6340 to 6360 step 0.12144538909196853\n",
      "Loss from 6360 to 6380 step 0.13699845746159553\n",
      "Loss from 6380 to 6400 step 0.13363942839205264\n",
      "Loss from 6400 to 6420 step 0.20302398540079594\n",
      "Loss from 6420 to 6440 step 0.14032953567802905\n",
      "Loss from 6440 to 6460 step 0.1263124655932188\n",
      "Loss from 6460 to 6480 step 0.15110748186707496\n",
      "Loss from 6480 to 6500 step 0.12126459553837776\n",
      "Loss from 6500 to 6520 step 0.13189345076680184\n",
      "Loss from 6520 to 6540 step 0.1559734083712101\n",
      "Loss from 6540 to 6560 step 0.2571167673915625\n",
      "Loss from 6560 to 6580 step 0.14467193856835364\n",
      "Loss from 6580 to 6600 step 0.16168970987200737\n",
      "Loss from 6600 to 6620 step 0.13806093595921992\n",
      "Loss from 6620 to 6640 step 0.12710502743721008\n",
      "Loss from 6640 to 6660 step 0.1320449396967888\n",
      "Loss from 6660 to 6680 step 0.15320393517613412\n",
      "Loss from 6680 to 6700 step 0.12531319372355937\n",
      "Loss from 6700 to 6720 step 0.11659897305071354\n",
      "Loss from 6720 to 6740 step 0.3652280494570732\n",
      "Loss from 6740 to 6760 step 0.12414729557931423\n",
      "Loss from 6760 to 6780 step 0.12429183349013329\n",
      "Loss from 6780 to 6800 step 0.1776456743478775\n",
      "Loss from 6800 to 6820 step 0.13975016996264458\n",
      "Loss from 6820 to 6840 step 0.1273487526923418\n",
      "Loss from 6840 to 6860 step 0.13351942412555218\n",
      "Loss from 6860 to 6880 step 0.12111203968524933\n",
      "Loss from 6880 to 6900 step 0.12436671331524848\n",
      "Loss from 6900 to 6920 step 0.12930946238338947\n",
      "Loss from 6920 to 6940 step 0.14300721809267997\n",
      "Loss from 6940 to 6960 step 0.12260606586933136\n",
      "Loss from 6960 to 6980 step 0.1327223215252161\n",
      "Loss from 6980 to 7000 step 0.15591816492378713\n",
      "Loss from 7000 to 7020 step 0.17327749766409398\n",
      "Loss from 7020 to 7040 step 0.13313487507402896\n",
      "Loss from 7040 to 7060 step 0.12662354335188866\n",
      "Loss from 7060 to 7080 step 0.14644136987626552\n",
      "Loss from 7080 to 7100 step 0.12686621770262718\n",
      "Loss from 7100 to 7120 step 0.1433397874236107\n",
      "Loss from 7120 to 7140 step 0.13679204136133194\n",
      "Loss from 7140 to 7160 step 0.12788658663630487\n",
      "Loss from 7160 to 7180 step 0.1266597095876932\n",
      "Loss from 7180 to 7200 step 0.12484104037284852\n",
      "Loss from 7200 to 7220 step 0.1276614774018526\n",
      "Loss from 7220 to 7240 step 0.12268046960234642\n",
      "Loss from 7240 to 7260 step 0.16360357627272606\n",
      "Loss from 7260 to 7280 step 0.1471029009670019\n",
      "Loss from 7280 to 7300 step 0.14191896952688693\n",
      "Loss from 7300 to 7320 step 0.11937707476317883\n",
      "Loss from 7320 to 7340 step 0.12323441430926323\n",
      "Loss from 7340 to 7360 step 0.14130967073142528\n",
      "Loss from 7360 to 7380 step 0.11988774836063384\n",
      "Loss from 7380 to 7400 step 0.13231589905917646\n",
      "Loss from 7400 to 7420 step 0.1586338948458433\n",
      "Loss from 7420 to 7440 step 0.12737621627748014\n",
      "Loss from 7440 to 7460 step 0.1301898118108511\n",
      "Loss from 7460 to 7480 step 0.16245462968945504\n",
      "Loss from 7480 to 7500 step 0.1446162097156048\n",
      "Loss from 7500 to 7520 step 0.14748349860310556\n",
      "Loss from 7520 to 7540 step 0.11991806402802467\n",
      "Loss from 7540 to 7560 step 0.12102028951048852\n",
      "Loss from 7560 to 7580 step 0.11403274238109588\n",
      "Loss from 7580 to 7600 step 0.15837962031364441\n",
      "Loss from 7600 to 7620 step 0.12874339297413825\n",
      "Loss from 7620 to 7640 step 0.11921906135976315\n",
      "Loss from 7640 to 7660 step 0.14918263219296932\n",
      "Loss from 7660 to 7680 step 0.14722161926329136\n",
      "Loss from 7680 to 7700 step 0.12551669031381607\n",
      "Loss from 7700 to 7720 step 0.14968760050833224\n",
      "Loss from 7720 to 7740 step 0.14171568006277085\n",
      "Loss from 7740 to 7760 step 0.13368196971714497\n",
      "Loss from 7760 to 7780 step 0.11849248446524144\n",
      "Loss from 7780 to 7800 step 0.1428038351237774\n",
      "Loss from 7800 to 7820 step 0.123552730306983\n",
      "Loss from 7820 to 7840 step 0.13849786892533303\n",
      "Loss from 7840 to 7860 step 0.11403592452406883\n",
      "Loss from 7860 to 7880 step 0.20908219926059246\n",
      "Loss from 7880 to 7900 step 0.12074189782142639\n",
      "Loss from 7900 to 7920 step 0.1564599361270666\n",
      "Loss from 7920 to 7940 step 0.11901582702994347\n",
      "Loss from 7940 to 7960 step 0.13307061940431594\n",
      "Loss from 7960 to 7980 step 0.11561719998717308\n",
      "Loss from 7980 to 8000 step 0.15017708465456964\n",
      "Loss from 8000 to 8020 step 0.12700316980481147\n",
      "Loss from 8020 to 8040 step 0.11862034164369106\n",
      "Loss from 8040 to 8060 step 0.11760497763752938\n",
      "Loss from 8060 to 8080 step 0.12044009938836098\n",
      "Loss from 8080 to 8100 step 0.1441630095243454\n",
      "Loss from 8100 to 8120 step 0.11249656975269318\n",
      "Loss from 8120 to 8140 step 0.1106917031109333\n",
      "Loss from 8140 to 8160 step 0.11534090712666512\n",
      "Loss from 8160 to 8180 step 0.2003909010440111\n",
      "Loss from 8180 to 8200 step 0.13128186538815498\n",
      "Loss from 8200 to 8220 step 0.14043415933847428\n",
      "Loss from 8220 to 8240 step 0.12038437351584434\n",
      "Loss from 8240 to 8260 step 0.11903709210455418\n",
      "Loss from 8260 to 8280 step 0.12524792291224002\n",
      "Loss from 8280 to 8300 step 0.15011509507894516\n",
      "Loss from 8300 to 8320 step 0.13773340098559855\n",
      "Loss from 8320 to 8340 step 0.11660698242485523\n",
      "Loss from 8340 to 8360 step 0.12273616343736649\n",
      "Loss from 8360 to 8380 step 0.14884359277784825\n",
      "Loss from 8380 to 8400 step 0.12194286808371543\n",
      "Loss from 8400 to 8420 step 0.12328314632177353\n",
      "Loss from 8420 to 8440 step 0.12448509745299816\n",
      "Loss from 8440 to 8460 step 0.13282492905855178\n",
      "Loss from 8460 to 8480 step 0.11825219094753266\n",
      "Loss from 8480 to 8500 step 0.12105275988578797\n",
      "Loss from 8500 to 8520 step 0.1234500415623188\n",
      "Loss from 8520 to 8540 step 0.11844603642821312\n",
      "Loss from 8540 to 8560 step 0.1251395892351866\n",
      "Loss from 8560 to 8580 step 0.11660972870886326\n",
      "Loss from 8580 to 8600 step 0.14046168848872184\n",
      "Loss from 8600 to 8620 step 0.11857405826449394\n",
      "Loss from 8620 to 8640 step 0.1793191559612751\n",
      "Loss from 8640 to 8660 step 0.12000278569757938\n",
      "Loss from 8660 to 8680 step 0.13205965794622898\n",
      "Loss from 8680 to 8700 step 0.15124523937702178\n",
      "Loss from 8700 to 8720 step 0.1416558124125004\n",
      "Loss from 8720 to 8740 step 0.128130866214633\n",
      "Loss from 8740 to 8760 step 0.2110072284936905\n",
      "Loss from 8760 to 8780 step 0.11897176615893841\n",
      "Loss from 8780 to 8800 step 0.13773157075047493\n",
      "Loss from 8800 to 8820 step 0.12027921751141549\n",
      "Loss from 8820 to 8840 step 0.12271444499492645\n",
      "Loss from 8840 to 8860 step 0.12849155403673648\n",
      "Loss from 8860 to 8880 step 0.11822842024266719\n",
      "Loss from 8880 to 8900 step 0.11682922765612602\n",
      "Loss from 8900 to 8920 step 0.12030189894139767\n",
      "Loss from 8920 to 8940 step 0.12983417212963105\n",
      "Loss from 8940 to 8960 step 0.12464133016765118\n",
      "Loss from 8960 to 8980 step 0.11481829434633255\n",
      "Loss from 8980 to 9000 step 0.13130188174545765\n",
      "Loss from 9000 to 9020 step 0.11952857859432697\n",
      "Loss from 9020 to 9040 step 0.12538650408387184\n",
      "Loss from 9040 to 9060 step 0.12268653288483619\n",
      "Loss from 9060 to 9080 step 0.11288609243929386\n",
      "Loss from 9080 to 9100 step 0.12043197080492973\n",
      "Loss from 9100 to 9120 step 0.13250769339501858\n",
      "Loss from 9120 to 9140 step 0.1293477300554514\n",
      "Loss from 9140 to 9160 step 0.13207954913377762\n",
      "Loss from 9160 to 9180 step 0.14810841269791125\n",
      "Loss from 9180 to 9200 step 0.143891092389822\n",
      "Loss from 9200 to 9220 step 0.13658373057842255\n",
      "Loss from 9220 to 9240 step 0.14842136353254318\n",
      "Loss from 9240 to 9260 step 0.13878883719444274\n",
      "Loss from 9260 to 9280 step 0.12163832560181617\n",
      "Loss from 9280 to 9300 step 0.12699679844081402\n",
      "Loss from 9300 to 9320 step 0.15064000636339187\n",
      "Loss from 9320 to 9340 step 0.14657338559627534\n",
      "Loss from 9340 to 9360 step 0.12257780954241752\n",
      "Loss from 9360 to 9380 step 0.12461989521980285\n",
      "Loss from 9380 to 9400 step 0.10801199339330196\n",
      "Loss from 9400 to 9420 step 0.11520373709499836\n",
      "Loss from 9420 to 9440 step 0.11857367604970932\n",
      "Loss from 9440 to 9460 step 0.1345189604908228\n",
      "Loss from 9460 to 9480 step 0.12032544389367103\n",
      "Loss from 9480 to 9500 step 0.11946670077741146\n",
      "Loss from 9500 to 9520 step 0.14141787216067314\n",
      "Loss from 9520 to 9540 step 0.10942795574665069\n",
      "Loss from 9540 to 9560 step 0.11821273416280746\n",
      "Loss from 9560 to 9580 step 0.12607253715395927\n",
      "Loss from 9580 to 9600 step 0.10910366848111153\n",
      "Loss from 9600 to 9620 step 0.12569162771105766\n",
      "Loss from 9620 to 9640 step 0.14941887147724628\n",
      "Loss from 9640 to 9660 step 0.1245753936469555\n",
      "Loss from 9660 to 9680 step 0.10539402812719345\n",
      "Loss from 9680 to 9700 step 0.11168415397405625\n",
      "Loss from 9700 to 9720 step 0.10788764841854573\n",
      "Loss from 9720 to 9740 step 0.12992184571921825\n",
      "Loss from 9740 to 9760 step 0.118949930742383\n",
      "Loss from 9760 to 9780 step 0.11310351826250553\n",
      "Loss from 9780 to 9800 step 0.12200675681233406\n",
      "Loss from 9800 to 9820 step 0.13342664130032061\n",
      "Loss from 9820 to 9840 step 0.1331168245524168\n",
      "Loss from 9840 to 9860 step 0.12601053044199945\n",
      "Loss from 9860 to 9880 step 0.15018100179731847\n",
      "Loss from 9880 to 9900 step 0.12437344714999199\n",
      "Loss from 9900 to 9920 step 0.1533834606409073\n",
      "Loss from 9920 to 9940 step 0.12603695578873159\n",
      "Loss from 9940 to 9960 step 0.12351738922297954\n",
      "Loss from 9960 to 9980 step 0.13619614616036416\n",
      "Loss from 9980 to 10000 step 0.12301679775118828\n",
      "Loss from 10000 to 10020 step 0.12093107923865318\n",
      "Loss from 10020 to 10040 step 0.11313492804765701\n",
      "Loss from 10040 to 10060 step 0.11483445279300213\n",
      "Loss from 10060 to 10080 step 0.12492559105157852\n",
      "Loss from 10080 to 10100 step 0.1136365320533514\n",
      "Loss from 10100 to 10120 step 0.11678755283355713\n",
      "Loss from 10120 to 10140 step 0.1336659535765648\n",
      "Loss from 10140 to 10160 step 0.12002943344414234\n",
      "Loss from 10160 to 10180 step 0.11748710870742798\n",
      "Loss from 10180 to 10200 step 0.12554114498198032\n",
      "Loss from 10200 to 10220 step 0.118588375300169\n",
      "Loss from 10220 to 10240 step 0.1225177064538002\n",
      "Loss from 10240 to 10260 step 0.12008320242166519\n",
      "Loss from 10260 to 10280 step 0.11522951349616051\n",
      "Loss from 10280 to 10300 step 0.13113098740577697\n",
      "Loss from 10300 to 10320 step 0.13366762027144433\n",
      "Loss from 10320 to 10340 step 0.10923889987170696\n",
      "Loss from 10340 to 10360 step 0.11150108687579632\n",
      "Loss from 10360 to 10380 step 0.11202028170228004\n",
      "Loss from 10380 to 10400 step 0.11880696676671505\n",
      "Loss from 10400 to 10420 step 0.11217803880572319\n",
      "Loss from 10420 to 10440 step 0.10976811349391938\n",
      "Loss from 10440 to 10460 step 0.11405467838048935\n",
      "Loss from 10460 to 10480 step 0.11436123736202716\n",
      "Loss from 10480 to 10500 step 0.10791455321013928\n",
      "Loss from 10500 to 10520 step 0.12647570669651031\n",
      "Loss from 10520 to 10540 step 0.3130190271884203\n",
      "Loss from 10540 to 10560 step 0.1388660714030266\n",
      "Loss from 10560 to 10580 step 0.13741861321032048\n",
      "Loss from 10580 to 10600 step 0.13426915630698205\n",
      "Loss from 10600 to 10620 step 0.11376927010715007\n",
      "Loss from 10620 to 10640 step 0.10009198486804963\n",
      "Loss from 10640 to 10660 step 0.11665815599262715\n",
      "Loss from 10660 to 10680 step 0.1329515092074871\n",
      "Loss from 10680 to 10700 step 0.10592001602053643\n",
      "Loss from 10700 to 10720 step 0.1346193380653858\n",
      "Loss from 10720 to 10740 step 0.14834224097430707\n",
      "Loss from 10740 to 10760 step 0.11579553410410881\n",
      "Loss from 10760 to 10780 step 0.11682712510228158\n",
      "Loss from 10780 to 10800 step 0.10819503031671047\n",
      "Loss from 10800 to 10820 step 0.18179011568427086\n",
      "Loss from 10820 to 10840 step 0.11271778717637063\n",
      "Loss from 10840 to 10860 step 0.1058859784156084\n",
      "Loss from 10860 to 10880 step 0.10511890016496181\n",
      "Loss from 10880 to 10900 step 0.11027126647531986\n",
      "Loss from 10900 to 10920 step 0.11382501758635044\n",
      "Loss from 10920 to 10940 step 0.12773516736924648\n",
      "Loss from 10940 to 10960 step 0.1090631939470768\n",
      "Loss from 10960 to 10980 step 0.14457543864846228\n",
      "Loss from 10980 to 11000 step 0.14228507578372956\n",
      "Loss from 11000 to 11020 step 0.19169244170188904\n",
      "Loss from 11020 to 11040 step 0.10930562913417816\n",
      "Loss from 11040 to 11060 step 0.10269393622875214\n",
      "Loss from 11060 to 11080 step 0.12032759562134743\n",
      "Loss from 11080 to 11100 step 0.13477579280734062\n",
      "Loss from 11100 to 11120 step 0.11325953602790832\n",
      "Loss from 11120 to 11140 step 0.11033502295613289\n",
      "Loss from 11140 to 11160 step 0.12997475937008857\n",
      "Loss from 11160 to 11180 step 0.11933375261723995\n",
      "Loss from 11180 to 11200 step 0.11369369812309742\n",
      "Loss from 11200 to 11220 step 0.10918746516108513\n",
      "Loss from 11220 to 11240 step 0.1090239379554987\n",
      "Loss from 11240 to 11260 step 0.13904318846762181\n",
      "Loss from 11260 to 11280 step 0.11655084379017353\n",
      "Loss from 11280 to 11300 step 0.11202077120542527\n",
      "Loss from 11300 to 11320 step 0.10302058644592763\n",
      "Loss from 11320 to 11340 step 0.10840675160288811\n",
      "Loss from 11340 to 11360 step 0.10727478228509427\n",
      "Loss from 11360 to 11380 step 0.15936336778104304\n",
      "Loss from 11380 to 11400 step 0.12424473837018013\n",
      "Loss from 11400 to 11420 step 0.10900518484413624\n",
      "Loss from 11420 to 11440 step 0.14419503509998322\n",
      "Loss from 11440 to 11460 step 0.13864606693387033\n",
      "Loss from 11460 to 11480 step 0.10868307314813137\n",
      "Loss from 11480 to 11500 step 0.11289576850831509\n",
      "Loss from 11500 to 11520 step 0.11082497276365758\n",
      "Loss from 11520 to 11540 step 0.1104432214051485\n",
      "Loss from 11540 to 11560 step 0.13051371090114117\n",
      "Loss from 11560 to 11580 step 0.11077341362833977\n",
      "Loss from 11580 to 11600 step 0.11066797748208046\n",
      "Loss from 11600 to 11620 step 0.1006322018802166\n",
      "Loss from 11620 to 11640 step 0.2328289721161127\n",
      "Loss from 11640 to 11660 step 0.10477288216352462\n",
      "Loss from 11660 to 11680 step 0.14559340551495553\n",
      "Loss from 11680 to 11700 step 0.15011682212352753\n",
      "Loss from 11700 to 11720 step 0.238221550360322\n",
      "Loss from 11720 to 11740 step 0.11244583167135716\n",
      "Loss from 11740 to 11760 step 0.1740147452801466\n",
      "Loss from 11760 to 11780 step 0.11075102984905243\n",
      "Loss from 11780 to 11800 step 0.12525367960333825\n",
      "Loss from 11800 to 11820 step 0.10839730091392993\n",
      "Loss from 11820 to 11840 step 0.1059485275298357\n",
      "Loss from 11840 to 11860 step 0.1358143225312233\n",
      "Loss from 11860 to 11880 step 0.12794910483062266\n",
      "Loss from 11880 to 11900 step 0.11541566550731659\n",
      "Loss from 11900 to 11920 step 0.149148877710104\n",
      "Loss from 11920 to 11940 step 0.10484136901795864\n",
      "Loss from 11940 to 11960 step 0.13495071940124034\n",
      "Loss from 11960 to 11980 step 0.1213843734934926\n",
      "Loss from 11980 to 12000 step 0.11774230860173703\n",
      "Loss from 12000 to 12020 step 0.11784741021692753\n",
      "Loss from 12020 to 12040 step 0.10315379351377488\n",
      "Loss from 12040 to 12060 step 0.11844690218567848\n",
      "Loss from 12060 to 12080 step 0.1179818145930767\n",
      "Loss from 12080 to 12100 step 0.13668988943099974\n",
      "Loss from 12100 to 12120 step 0.10842223316431046\n",
      "Loss from 12120 to 12140 step 0.10269612446427345\n",
      "Loss from 12140 to 12160 step 0.11590854562819004\n",
      "Loss from 12160 to 12180 step 0.12973986230790616\n",
      "Loss from 12180 to 12200 step 0.11178988590836525\n",
      "Loss from 12200 to 12220 step 0.11874370090663433\n",
      "Loss from 12220 to 12240 step 0.10788366310298443\n",
      "Loss from 12240 to 12260 step 0.12950948998332024\n",
      "Loss from 12260 to 12280 step 0.14201840683817862\n",
      "Loss from 12280 to 12300 step 0.107871900126338\n",
      "Loss from 12300 to 12320 step 0.10855971910059452\n",
      "Loss from 12320 to 12340 step 0.11150310412049294\n",
      "Loss from 12340 to 12360 step 0.11183308511972427\n",
      "Loss from 12360 to 12380 step 0.09692038521170616\n",
      "Loss from 12380 to 12400 step 0.14625172987580298\n",
      "Loss from 12400 to 12420 step 0.1443379446864128\n",
      "Loss from 12420 to 12440 step 0.1262330211699009\n",
      "Loss from 12440 to 12460 step 0.09619457218796015\n",
      "Loss from 12460 to 12480 step 0.10179265998303891\n",
      "Loss from 12480 to 12500 step 0.1383803144097328\n",
      "Train loss: 0.1591179605549574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [01:58<00:00, 845.00 examples/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad466668a9304413adcd7274d6f28329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss from 0 to 20 step 0.10285154841840267\n",
      "Loss from 20 to 40 step 0.11080565750598907\n",
      "Loss from 40 to 60 step 0.3078573767095804\n",
      "Loss from 60 to 80 step 0.10265103206038476\n",
      "Loss from 80 to 100 step 0.22896673195064068\n",
      "Loss from 100 to 120 step 0.11339891590178013\n",
      "Loss from 120 to 140 step 0.118510040640831\n",
      "Loss from 140 to 160 step 0.10684010088443756\n",
      "Loss from 160 to 180 step 0.10796084329485893\n",
      "Loss from 180 to 200 step 0.10797228924930095\n",
      "Loss from 200 to 220 step 0.12071321345865726\n",
      "Loss from 220 to 240 step 0.10634805709123611\n",
      "Loss from 240 to 260 step 0.144910204783082\n",
      "Loss from 260 to 280 step 0.1301879409700632\n",
      "Loss from 280 to 300 step 0.1127145305275917\n",
      "Loss from 300 to 320 step 0.09767359867691994\n",
      "Loss from 320 to 340 step 0.1087566576898098\n",
      "Loss from 340 to 360 step 0.10986167304217816\n",
      "Loss from 360 to 380 step 0.11803590245544911\n",
      "Loss from 380 to 400 step 0.13229920230805875\n",
      "Loss from 400 to 420 step 0.18985508047044278\n",
      "Loss from 420 to 440 step 0.12711454294621943\n",
      "Loss from 440 to 460 step 0.1438758734613657\n",
      "Loss from 460 to 480 step 0.10002466961741448\n",
      "Loss from 480 to 500 step 0.11780091971158982\n",
      "Loss from 500 to 520 step 0.2496130596846342\n",
      "Loss from 520 to 540 step 0.11010137349367141\n",
      "Loss from 540 to 560 step 0.11847028657793998\n",
      "Loss from 560 to 580 step 0.09909360185265541\n",
      "Loss from 580 to 600 step 0.12815227098762988\n",
      "Loss from 600 to 620 step 0.11824702955782414\n",
      "Loss from 620 to 640 step 0.1384471595287323\n",
      "Loss from 640 to 660 step 0.10116280987858772\n",
      "Loss from 660 to 680 step 0.10105013325810433\n",
      "Loss from 680 to 700 step 0.11629152894020081\n",
      "Loss from 700 to 720 step 0.10011147856712341\n",
      "Loss from 720 to 740 step 0.10492247976362705\n",
      "Loss from 740 to 760 step 0.0956626333296299\n",
      "Loss from 760 to 780 step 0.12168981023132801\n",
      "Loss from 780 to 800 step 0.13486646264791488\n",
      "Loss from 800 to 820 step 0.10866466350853443\n",
      "Loss from 820 to 840 step 0.11861868910491466\n",
      "Loss from 840 to 860 step 0.11691351346671582\n",
      "Loss from 860 to 880 step 0.10347263365983964\n",
      "Loss from 880 to 900 step 0.12268399856984616\n",
      "Loss from 900 to 920 step 0.11078397370874882\n",
      "Loss from 920 to 940 step 0.13767734952270985\n",
      "Loss from 940 to 960 step 0.33836731016635896\n",
      "Loss from 960 to 980 step 0.11088853627443314\n",
      "Loss from 980 to 1000 step 0.11361480131745338\n",
      "Loss from 1000 to 1020 step 0.10275621637701988\n",
      "Loss from 1020 to 1040 step 0.10619919523596763\n",
      "Loss from 1040 to 1060 step 0.09829567931592464\n",
      "Loss from 1060 to 1080 step 0.1869253534823656\n",
      "Loss from 1080 to 1100 step 0.15148746334016322\n",
      "Loss from 1100 to 1120 step 0.1302906706929207\n",
      "Loss from 1120 to 1140 step 0.09614530764520168\n",
      "Loss from 1140 to 1160 step 0.09561107121407986\n",
      "Loss from 1160 to 1180 step 0.11376079209148884\n",
      "Loss from 1180 to 1200 step 0.09929992891848087\n",
      "Loss from 1200 to 1220 step 0.10302688144147396\n",
      "Loss from 1220 to 1240 step 0.10292154774069787\n",
      "Loss from 1240 to 1260 step 0.11712450347840786\n",
      "Loss from 1260 to 1280 step 0.11018550284206867\n",
      "Loss from 1280 to 1300 step 0.12006650120019913\n",
      "Loss from 1300 to 1320 step 0.12241836600005626\n",
      "Loss from 1320 to 1340 step 0.13383958227932452\n",
      "Loss from 1340 to 1360 step 0.10156674459576606\n",
      "Loss from 1360 to 1380 step 0.10859789475798606\n",
      "Loss from 1380 to 1400 step 0.10187678188085555\n",
      "Loss from 1400 to 1420 step 0.15467131920158864\n",
      "Loss from 1420 to 1440 step 0.13713272362947465\n",
      "Loss from 1440 to 1460 step 0.18563563823699952\n",
      "Loss from 1460 to 1480 step 0.4547961689531803\n",
      "Loss from 1480 to 1500 step 0.1114941656589508\n",
      "Loss from 1500 to 1520 step 0.10015104487538337\n",
      "Loss from 1520 to 1540 step 0.09984021671116353\n",
      "Loss from 1540 to 1560 step 0.09963381811976432\n",
      "Loss from 1560 to 1580 step 0.1081620492041111\n",
      "Loss from 1580 to 1600 step 0.1067117065191269\n",
      "Loss from 1600 to 1620 step 0.09681911543011665\n",
      "Loss from 1620 to 1640 step 0.10947327278554439\n",
      "Loss from 1640 to 1660 step 0.10024610236287117\n",
      "Loss from 1660 to 1680 step 0.11192226819694043\n",
      "Loss from 1680 to 1700 step 0.1040250726044178\n",
      "Loss from 1700 to 1720 step 0.09798963442444801\n",
      "Loss from 1720 to 1740 step 0.11050324290990829\n",
      "Loss from 1740 to 1760 step 0.15294618532061577\n",
      "Loss from 1760 to 1780 step 0.12967143952846527\n",
      "Loss from 1780 to 1800 step 0.11413028873503209\n",
      "Loss from 1800 to 1820 step 0.11758162975311279\n",
      "Loss from 1820 to 1840 step 0.1302338857203722\n",
      "Loss from 1840 to 1860 step 0.10005879551172256\n",
      "Loss from 1860 to 1880 step 0.09620144665241241\n",
      "Loss from 1880 to 1900 step 0.10837527364492416\n",
      "Loss from 1900 to 1920 step 0.12117615118622779\n",
      "Loss from 1920 to 1940 step 0.14400829263031484\n",
      "Loss from 1940 to 1960 step 0.11206877790391445\n",
      "Loss from 1960 to 1980 step 0.10367105640470982\n",
      "Loss from 1980 to 2000 step 0.10097859092056752\n",
      "Loss from 2000 to 2020 step 0.11458533443510532\n",
      "Loss from 2020 to 2040 step 0.10851185098290443\n",
      "Loss from 2040 to 2060 step 0.15944119691848754\n",
      "Loss from 2060 to 2080 step 0.11733898632228375\n",
      "Loss from 2080 to 2100 step 0.09694082960486412\n",
      "Loss from 2100 to 2120 step 0.14878094084560872\n",
      "Loss from 2120 to 2140 step 0.10324463434517384\n",
      "Loss from 2140 to 2160 step 0.13314023651182652\n",
      "Loss from 2160 to 2180 step 0.11119048409163952\n",
      "Loss from 2180 to 2200 step 0.11188693717122078\n",
      "Loss from 2200 to 2220 step 0.10051269307732583\n",
      "Loss from 2220 to 2240 step 0.09863322861492634\n",
      "Loss from 2240 to 2260 step 0.09968036487698555\n",
      "Loss from 2260 to 2280 step 0.11521936729550361\n",
      "Loss from 2280 to 2300 step 0.10734670385718345\n",
      "Loss from 2300 to 2320 step 0.09155693091452122\n",
      "Loss from 2320 to 2340 step 0.13932313695549964\n",
      "Loss from 2340 to 2360 step 0.10963910520076751\n",
      "Loss from 2360 to 2380 step 0.10835968218743801\n",
      "Loss from 2380 to 2400 step 0.10580588355660439\n",
      "Loss from 2400 to 2420 step 0.14058385826647282\n",
      "Loss from 2420 to 2440 step 0.09845645502209663\n",
      "Loss from 2440 to 2460 step 0.11493410207331181\n",
      "Loss from 2460 to 2480 step 0.10496580973267555\n",
      "Loss from 2480 to 2500 step 0.11687804311513901\n",
      "Loss from 2500 to 2520 step 0.10826771929860116\n",
      "Loss from 2520 to 2540 step 0.10557436868548394\n",
      "Loss from 2540 to 2560 step 0.11032732352614402\n",
      "Loss from 2560 to 2580 step 0.10865007191896439\n",
      "Loss from 2580 to 2600 step 0.09858740605413914\n",
      "Loss from 2600 to 2620 step 0.10047689490020276\n",
      "Loss from 2620 to 2640 step 0.11929057836532593\n",
      "Loss from 2640 to 2660 step 0.10611463896930218\n",
      "Loss from 2660 to 2680 step 0.14841100424528123\n",
      "Loss from 2680 to 2700 step 0.11512073501944542\n",
      "Loss from 2700 to 2720 step 0.1384567476809025\n",
      "Loss from 2720 to 2740 step 0.12100371606647968\n",
      "Loss from 2740 to 2760 step 0.10226036161184311\n",
      "Loss from 2760 to 2780 step 0.10288236998021602\n",
      "Loss from 2780 to 2800 step 0.10142469741404056\n",
      "Loss from 2800 to 2820 step 0.11452478021383286\n",
      "Loss from 2820 to 2840 step 0.10966026037931442\n",
      "Loss from 2840 to 2860 step 0.12778007835149766\n",
      "Loss from 2860 to 2880 step 0.09965997859835625\n",
      "Loss from 2880 to 2900 step 0.123933419957757\n",
      "Loss from 2900 to 2920 step 0.10797518901526929\n",
      "Loss from 2920 to 2940 step 0.14791995324194432\n",
      "Loss from 2940 to 2960 step 0.10038748905062675\n",
      "Loss from 2960 to 2980 step 0.1305296216160059\n",
      "Loss from 2980 to 3000 step 0.1153734140098095\n",
      "Loss from 3000 to 3020 step 0.11812106519937515\n",
      "Loss from 3020 to 3040 step 0.10312237478792667\n",
      "Loss from 3040 to 3060 step 0.11422399282455445\n",
      "Loss from 3060 to 3080 step 0.16783765368163586\n",
      "Loss from 3080 to 3100 step 0.10603919588029384\n",
      "Loss from 3100 to 3120 step 0.1178994506597519\n",
      "Loss from 3120 to 3140 step 0.11030337736010551\n",
      "Loss from 3140 to 3160 step 0.09426622614264488\n",
      "Loss from 3160 to 3180 step 0.102223090082407\n",
      "Loss from 3180 to 3200 step 0.11226214654743671\n",
      "Loss from 3200 to 3220 step 0.10085620656609535\n",
      "Loss from 3220 to 3240 step 0.1397395357489586\n",
      "Loss from 3240 to 3260 step 0.12243518382310867\n",
      "Loss from 3260 to 3280 step 0.1664732351899147\n",
      "Loss from 3280 to 3300 step 0.11997809149324894\n",
      "Loss from 3300 to 3320 step 0.09851647764444352\n",
      "Loss from 3320 to 3340 step 0.1158129770308733\n",
      "Loss from 3340 to 3360 step 0.21407221183180808\n",
      "Loss from 3360 to 3380 step 0.11263796836137771\n",
      "Loss from 3380 to 3400 step 0.10123453214764595\n",
      "Loss from 3400 to 3420 step 0.14976103529334067\n",
      "Loss from 3420 to 3440 step 0.09488984644412994\n",
      "Loss from 3440 to 3460 step 0.09672305397689343\n",
      "Loss from 3460 to 3480 step 0.11265819184482098\n",
      "Loss from 3480 to 3500 step 0.16598878428339958\n",
      "Loss from 3500 to 3520 step 0.10253278650343418\n",
      "Loss from 3520 to 3540 step 0.10865186713635921\n",
      "Loss from 3540 to 3560 step 0.10319320186972618\n",
      "Loss from 3560 to 3580 step 0.11218544505536557\n",
      "Loss from 3580 to 3600 step 0.12299324683845043\n",
      "Loss from 3600 to 3620 step 0.12108914740383625\n",
      "Loss from 3620 to 3640 step 0.0979474563151598\n",
      "Loss from 3640 to 3660 step 0.1434098407626152\n",
      "Loss from 3660 to 3680 step 0.11049110479652882\n",
      "Loss from 3680 to 3700 step 0.11023947447538376\n",
      "Loss from 3700 to 3720 step 0.09774627350270748\n",
      "Loss from 3720 to 3740 step 0.09780971482396125\n",
      "Loss from 3740 to 3760 step 0.0931754820048809\n",
      "Loss from 3760 to 3780 step 0.11239796131849289\n",
      "Loss from 3780 to 3800 step 0.10762159042060375\n",
      "Loss from 3800 to 3820 step 0.11243251152336597\n",
      "Loss from 3820 to 3840 step 0.11740404181182384\n",
      "Loss from 3840 to 3860 step 0.11364435255527497\n",
      "Loss from 3860 to 3880 step 0.09795983545482159\n",
      "Loss from 3880 to 3900 step 0.2577352602034807\n",
      "Loss from 3900 to 3920 step 0.11843431331217288\n",
      "Loss from 3920 to 3940 step 0.11284951008856296\n",
      "Loss from 3940 to 3960 step 0.10850757136940956\n",
      "Loss from 3960 to 3980 step 0.281758888438344\n",
      "Loss from 3980 to 4000 step 0.13087448365986348\n",
      "Loss from 4000 to 4020 step 0.12126341611146926\n",
      "Loss from 4020 to 4040 step 0.10944287180900573\n",
      "Loss from 4040 to 4060 step 0.10371984355151653\n",
      "Loss from 4060 to 4080 step 0.12042225636541844\n",
      "Loss from 4080 to 4100 step 0.11487673074007035\n",
      "Loss from 4100 to 4120 step 0.11567818485200405\n",
      "Loss from 4120 to 4140 step 0.10442242436110974\n",
      "Loss from 4140 to 4160 step 0.11476718671619893\n",
      "Loss from 4160 to 4180 step 0.0991981454193592\n",
      "Loss from 4180 to 4200 step 0.10543000921607018\n",
      "Loss from 4200 to 4220 step 0.09783543832600117\n",
      "Loss from 4220 to 4240 step 0.11130682080984115\n",
      "Loss from 4240 to 4260 step 0.1007223255932331\n",
      "Loss from 4260 to 4280 step 0.10950239561498165\n",
      "Loss from 4280 to 4300 step 0.1152008105069399\n",
      "Loss from 4300 to 4320 step 0.12174170054495334\n",
      "Loss from 4320 to 4340 step 0.11588845811784268\n",
      "Loss from 4340 to 4360 step 0.12531327307224274\n",
      "Loss from 4360 to 4380 step 0.09802265018224716\n",
      "Loss from 4380 to 4400 step 0.11194522269070148\n",
      "Loss from 4400 to 4420 step 0.10560332499444484\n",
      "Loss from 4420 to 4440 step 0.11935116052627563\n",
      "Loss from 4440 to 4460 step 0.11028354242444038\n",
      "Loss from 4460 to 4480 step 0.10977374278008938\n",
      "Loss from 4480 to 4500 step 0.10268019512295723\n",
      "Loss from 4500 to 4520 step 0.12850746624171733\n",
      "Loss from 4520 to 4540 step 0.1279801048338413\n",
      "Loss from 4540 to 4560 step 0.09535226672887802\n",
      "Loss from 4560 to 4580 step 0.14125670604407786\n",
      "Loss from 4580 to 4600 step 0.12930538207292558\n",
      "Loss from 4600 to 4620 step 0.10646666102111339\n",
      "Loss from 4620 to 4640 step 0.1050310093909502\n",
      "Loss from 4640 to 4660 step 0.09855248928070068\n",
      "Loss from 4660 to 4680 step 0.09972879812121391\n",
      "Loss from 4680 to 4700 step 0.10875155068933964\n",
      "Loss from 4700 to 4720 step 0.10566342361271382\n",
      "Loss from 4720 to 4740 step 0.09666336216032505\n",
      "Loss from 4740 to 4760 step 0.11950708404183388\n",
      "Loss from 4760 to 4780 step 0.12209771797060967\n",
      "Loss from 4780 to 4800 step 0.11253000758588314\n",
      "Loss from 4800 to 4820 step 0.10059184208512306\n",
      "Loss from 4820 to 4840 step 0.10284445397555828\n",
      "Loss from 4840 to 4860 step 0.13506174013018607\n",
      "Loss from 4860 to 4880 step 0.09951161928474903\n",
      "Loss from 4880 to 4900 step 0.10877087861299514\n",
      "Loss from 4900 to 4920 step 0.12428477443754674\n",
      "Loss from 4920 to 4940 step 0.10604026578366757\n",
      "Loss from 4940 to 4960 step 0.09738278426229954\n",
      "Loss from 4960 to 4980 step 0.11200012676417828\n",
      "Loss from 4980 to 5000 step 0.10338297411799431\n",
      "Loss from 5000 to 5020 step 0.11307701244950294\n",
      "Loss from 5020 to 5040 step 0.1596622284501791\n",
      "Loss from 5040 to 5060 step 0.11143068559467792\n",
      "Loss from 5060 to 5080 step 0.10737587846815586\n",
      "Loss from 5080 to 5100 step 0.09699535518884658\n",
      "Loss from 5100 to 5120 step 0.12496393918991089\n",
      "Loss from 5120 to 5140 step 0.10355073399841785\n",
      "Loss from 5140 to 5160 step 0.1424109485000372\n",
      "Loss from 5160 to 5180 step 0.12210959568619728\n",
      "Loss from 5180 to 5200 step 0.12378795482218266\n",
      "Loss from 5200 to 5220 step 0.13263540342450142\n",
      "Loss from 5220 to 5240 step 0.2356533885002136\n",
      "Loss from 5240 to 5260 step 0.102999022975564\n",
      "Loss from 5260 to 5280 step 0.1556356582790613\n",
      "Loss from 5280 to 5300 step 0.25529367588460444\n",
      "Loss from 5300 to 5320 step 0.122483029961586\n",
      "Loss from 5320 to 5340 step 0.11004800982773304\n",
      "Loss from 5340 to 5360 step 0.0946993887424469\n",
      "Loss from 5360 to 5380 step 0.09921549856662751\n",
      "Loss from 5380 to 5400 step 0.09966202899813652\n",
      "Loss from 5400 to 5420 step 0.10257169343531132\n",
      "Loss from 5420 to 5440 step 0.10814411919564008\n",
      "Loss from 5440 to 5460 step 0.10528805665671825\n",
      "Loss from 5460 to 5480 step 0.10609589777886867\n",
      "Loss from 5480 to 5500 step 0.11584219336509705\n",
      "Loss from 5500 to 5520 step 0.10568773820996284\n",
      "Loss from 5520 to 5540 step 0.10387411005795003\n",
      "Loss from 5540 to 5560 step 0.09907045736908912\n",
      "Loss from 5560 to 5580 step 0.10569404251873493\n",
      "Loss from 5580 to 5600 step 0.1040595255792141\n",
      "Loss from 5600 to 5620 step 0.10655088871717452\n",
      "Loss from 5620 to 5640 step 0.10656712539494037\n",
      "Loss from 5640 to 5660 step 0.11086087636649608\n",
      "Loss from 5660 to 5680 step 0.11544384732842446\n",
      "Loss from 5680 to 5700 step 0.09786009043455124\n",
      "Loss from 5700 to 5720 step 0.11861027739942073\n",
      "Loss from 5720 to 5740 step 0.11004543378949165\n",
      "Loss from 5740 to 5760 step 0.098013536632061\n",
      "Loss from 5760 to 5780 step 0.10746632479131221\n",
      "Loss from 5780 to 5800 step 0.11457964442670346\n",
      "Loss from 5800 to 5820 step 0.11161734573543072\n",
      "Loss from 5820 to 5840 step 0.11042946875095368\n",
      "Loss from 5840 to 5860 step 0.1265692662447691\n",
      "Loss from 5860 to 5880 step 0.11770008355379105\n",
      "Loss from 5880 to 5900 step 0.12501313537359238\n",
      "Loss from 5900 to 5920 step 0.09628762453794479\n",
      "Loss from 5920 to 5940 step 0.31080575957894324\n",
      "Loss from 5940 to 5960 step 0.11364265903830528\n",
      "Loss from 5960 to 5980 step 0.14331155195832251\n",
      "Loss from 5980 to 6000 step 0.09611332230269909\n",
      "Loss from 6000 to 6020 step 0.10004563704133033\n",
      "Loss from 6020 to 6040 step 0.11438971757888794\n",
      "Loss from 6040 to 6060 step 0.10899064764380455\n",
      "Loss from 6060 to 6080 step 0.09883142076432705\n",
      "Loss from 6080 to 6100 step 0.11405881196260452\n",
      "Loss from 6100 to 6120 step 0.1094251036643982\n",
      "Loss from 6120 to 6140 step 0.10193873420357705\n",
      "Loss from 6140 to 6160 step 0.12744939103722572\n",
      "Loss from 6160 to 6180 step 0.13541353344917298\n",
      "Loss from 6180 to 6200 step 0.14660809747874737\n",
      "Loss from 6200 to 6220 step 0.14607120417058467\n",
      "Loss from 6220 to 6240 step 0.09800911955535412\n",
      "Loss from 6240 to 6260 step 0.0954938493669033\n",
      "Loss from 6260 to 6280 step 0.11260250806808472\n",
      "Loss from 6280 to 6300 step 0.09635342173278331\n",
      "Loss from 6300 to 6320 step 0.12329631820321083\n",
      "Loss from 6320 to 6340 step 0.09909717179834843\n",
      "Loss from 6340 to 6360 step 0.11552883721888066\n",
      "Loss from 6360 to 6380 step 0.09397672489285469\n",
      "Loss from 6380 to 6400 step 0.1027822844684124\n",
      "Loss from 6400 to 6420 step 0.10303553640842437\n",
      "Loss from 6420 to 6440 step 0.11647052653133869\n",
      "Loss from 6440 to 6460 step 0.11059482954442501\n",
      "Loss from 6460 to 6480 step 0.16793780736625194\n",
      "Loss from 6480 to 6500 step 0.10162231624126435\n",
      "Loss from 6500 to 6520 step 0.171022392436862\n",
      "Loss from 6520 to 6540 step 0.10684150308370591\n",
      "Loss from 6540 to 6560 step 0.10791554562747478\n",
      "Loss from 6560 to 6580 step 0.10276744440197945\n",
      "Loss from 6580 to 6600 step 0.11564997173845767\n",
      "Loss from 6600 to 6620 step 0.10442786663770676\n",
      "Loss from 6620 to 6640 step 0.09831177480518818\n",
      "Loss from 6640 to 6660 step 0.10910252928733825\n",
      "Loss from 6660 to 6680 step 0.09664486385881901\n",
      "Loss from 6680 to 6700 step 0.10510723479092121\n",
      "Loss from 6700 to 6720 step 0.11500728502869606\n",
      "Loss from 6720 to 6740 step 0.10344999097287655\n",
      "Loss from 6740 to 6760 step 0.11508042141795158\n",
      "Loss from 6760 to 6780 step 0.11198501326143742\n",
      "Loss from 6780 to 6800 step 0.09816721342504024\n",
      "Loss from 6800 to 6820 step 0.1270890861749649\n",
      "Loss from 6820 to 6840 step 0.1180313415825367\n",
      "Loss from 6840 to 6860 step 0.0961432833224535\n",
      "Loss from 6860 to 6880 step 0.3392355486750603\n",
      "Loss from 6880 to 6900 step 0.10410782285034656\n",
      "Loss from 6900 to 6920 step 0.1098378237336874\n",
      "Loss from 6920 to 6940 step 0.09394717402756214\n",
      "Loss from 6940 to 6960 step 0.11973254717886447\n",
      "Loss from 6960 to 6980 step 0.10280713140964508\n",
      "Loss from 6980 to 7000 step 0.11883293613791465\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7483/2525321657.py\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlog_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msave_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAD_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7483/919364136.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, n_epochs, pad_id, log_step, save_period, run, dataset)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train loss: {np.mean(train_loss)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7483/919364136.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, criterion, optimizer, train_loader, epoch, pad_id, log_step, save_period, run)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# batch_size, seq_len, vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "MAX_SEQ_LEN = 1024\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = LLaMa(vocab_size, 8, MAX_SEQ_LEN, 768, 8, device).to(device)\n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "n_epochs = 10\n",
    "log_step = 20\n",
    "save_period = 5000\n",
    "train(model, criterion, optimizer, n_epochs, PAD_ID, log_step, save_period, run, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T13:09:09.223038Z",
     "iopub.status.busy": "2024-11-09T13:09:09.221467Z",
     "iopub.status.idle": "2024-11-09T13:09:12.303113Z",
     "shell.execute_reply": "2024-11-09T13:09:12.301699Z",
     "shell.execute_reply.started": "2024-11-09T13:09:09.223002Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T13:11:35.048440Z",
     "iopub.status.busy": "2024-11-09T13:11:35.047222Z",
     "iopub.status.idle": "2024-11-09T13:11:35.120950Z",
     "shell.execute_reply": "2024-11-09T13:11:35.119490Z",
     "shell.execute_reply.started": "2024-11-09T13:11:35.048405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/work/resources/wandb/run-20241109_131105-mpmuoqkg/files/best_model.pth']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.save(\"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
